---
title: AgentSDK Technical Whitepaper
description: AgentSDK - 企业级AI Agent开发框架技术白皮书
navigation:
  icon: i-lucide-file-text
---

# AgentSDK: 企业级AI Agent开发框架

**基于Go语言的生产就绪型Agent运行时系统**

---

**版本**: v0.7.0
**日期**: 2025年11月
**作者**: WordflowLab Team
**机构**: WordflowLab

---

## Abstract（摘要）

随着大语言模型（LLM）技术的快速发展，AI Agent已成为构建智能应用的核心范式。然而，现有的Agent框架大多面临生产就绪性不足、性能瓶颈、安全风险等挑战。本文介绍AgentSDK，一个基于Go语言的企业级AI Agent开发框架，专注于解决生产环境中的实际问题。

**核心贡献**：

1. **事件驱动架构**：创新性的三通道设计（Progress/Control/Monitor），实现了实时进度追踪、人机交互审批和完整治理审计的分离关注点。

2. **洋葱模型中间件系统**：借鉴Web框架设计理念，实现了灵活的请求/响应拦截机制，支持文件系统、子Agent委派、自动总结、PII脱敏等功能的即插即用。

3. **三层记忆系统**：Text Memory、Working Memory和Semantic Memory的分层设计，配合Memory Provenance（内存溯源）、Memory Consolidation（内存合并）和质量分析，完整实现了Google Context Engineering白皮书的所有核心功能。

4. **生产级性能**：基于Go 1.23的iter.Seq2流式接口，内存占用降低80%+，支持100+并发Agent，Middleware Stack性能达到36.21 ns/op。

5. **企业级安全**：多Sandbox支持（Docker、阿里云、火山引擎）、10+种PII类型自动脱敏、完整的权限系统和审计日志，满足金融、医疗等行业的合规要求。

**关键性能指标**：

- **代码规模**: 34,603行核心代码 + 15,685行测试代码
- **测试覆盖率**: 80%+
- **性能**: Middleware Stack 36.21 ns/op，Backend Write 257.9 ns/op
- **并发能力**: 支持100+并发Agent
- **内存优化**: 相比传统实现降低80%+内存占用

AgentSDK已在多个生产环境中部署，支撑了客户服务、文档处理、自动化运维等多种企业级应用场景。

---

## 目录

1. [Introduction（引言）](#1-introduction引言)
2. [System Architecture（系统架构）](#2-system-architecture系统架构)
3. [Event-Driven Architecture（事件驱动架构）](#3-event-driven-architecture事件驱动架构)
4. [Middleware System（中间件系统）](#4-middleware-system中间件系统)
5. [Memory System（记忆系统）](#5-memory-system记忆系统)
6. [Context Engineering（上下文工程）](#6-context-engineering上下文工程)
7. [Security & Privacy（安全与隐私）](#7-security--privacy安全与隐私)
8. [Multi-Agent System（多Agent系统）](#8-multi-agent-system多agent系统)
9. [Provider & Tool System](#9-provider--tool-system)
10. [Skills System（技能系统）](#10-skills-system技能系统)
11. [Implementation Details（实现细节）](#11-implementation-details实现细节)
12. [Performance Evaluation（性能评估）](#12-performance-evaluation性能评估)
13. [Use Cases（使用场景）](#13-use-cases使用场景)
14. [Related Work（相关工作）](#14-related-work相关工作)
15. [Lessons Learned（经验总结）](#15-lessons-learned经验总结)
16. [Future Work（未来工作）](#16-future-work未来工作)
17. [Conclusion（结论）](#17-conclusion结论)
18. [References（参考文献）](#18-references参考文献)

---

## 1. Introduction（引言）

### 1.1 背景与挑战

大语言模型（LLM）的突破性进展为AI应用开发带来了革命性变化。然而，从模型调用到生产就绪的Agent系统之间存在巨大鸿沟：

**现有框架的局限性**：

1. **性能瓶颈**：Python生态主导的框架存在GIL限制，难以支撑高并发场景
2. **生产就绪性不足**：缺乏完善的错误处理、审计、权限控制等企业级特性
3. **安全风险**：PII泄露、工具调用失控、缺乏沙箱隔离
4. **上下文管理困难**：长对话场景下的Token溢出、记忆管理混乱
5. **可观测性缺失**：难以追踪Agent行为、诊断问题、优化性能

**企业级需求**：

- **高并发**：同时服务数千用户的能力
- **低延迟**：毫秒级响应时间
- **安全合规**：PII保护、审计日志、权限管理
- **可扩展性**：水平扩展能力、插件化架构
- **可观测性**：分布式追踪、指标收集、日志关联

### 1.2 设计目标

AgentSDK的设计围绕四个核心目标：

#### 1.2.1 生产就绪性

- **完整的错误处理**：优雅降级、自动重试、详细错误信息
- **审计与合规**：完整的操作日志、工具调用追踪、数据溯源
- **权限系统**：细粒度的工具级权限控制、HITL（Human-in-the-Loop）支持
- **可观测性**：OpenTelemetry集成、分布式追踪、实时监控

#### 1.2.2 高性能并发

- **Go语言优势**：Goroutine轻量级并发、零GC暂停、原生并发原语
- **流式API**：基于Go 1.23 iter.Seq2，内存占用降低80%+
- **零拷贝优化**：减少不必要的数据复制
- **连接池管理**：数据库连接、HTTP客户端的池化

#### 1.2.3 安全可控性

- **多层Sandbox**：Docker本地、云平台（阿里云、火山引擎）
- **PII自动脱敏**：10+种PII类型实时检测和脱敏
- **权限控制**：工具级allow/deny/ask配置
- **审批流程**：关键操作的人工审批

#### 1.2.4 架构清晰性

- **分层架构**：Agent层 → Backend层 → 基础设施层
- **接口优先**：Protocol + Factory模式实现依赖注入
- **组合优于继承**：通过接口组合实现功能扩展
- **显式优于隐式**：清晰的数据流和依赖关系

### 1.3 核心贡献

AgentSDK在以下方面做出了关键贡献：

#### 1.3.1 事件驱动架构创新

传统Agent框架采用单通道的消息传递，难以区分不同类型的事件。AgentSDK创新性地提出**三通道设计**：

- **Progress通道**：实时文本流、工具执行进度、思考过程
- **Control通道**：工具审批、用户输入、暂停/继续
- **Monitor通道**：审计事件、性能指标、错误追踪

这种分离关注点的设计使得：
- **实时性**：用户可以看到Agent的实时思考过程
- **可控性**：关键操作可以暂停等待人工审批
- **可观测性**：完整的行为追踪和审计日志

#### 1.3.2 洋葱模型中间件系统

借鉴Web框架（如Koa.js）的中间件设计，AgentSDK实现了**洋葱模型**：

```
Request → [MW1 before] → [MW2 before] → [MW3 before] → Core
Response ← [MW1 after] ← [MW2 after] ← [MW3 after] ← Core
```

优势：
- **即插即用**：中间件可以独立开发和测试
- **组合灵活**：通过优先级控制执行顺序
- **职责单一**：每个中间件专注一个功能

#### 1.3.3 Google Context Engineering完整实现

AgentSDK完整实现了Google "Context Engineering: Sessions, Memory" 白皮书的所有核心功能，评分**100/100**：

- ✅ **Memory Provenance**（内存溯源）：4种来源类型、置信度计算、谱系追踪
- ✅ **PII Auto-Redaction**（PII自动脱敏）：10+种PII类型、4种脱敏策略
- ✅ **Memory Consolidation**（内存合并）：冗余合并、冲突解决、LLM驱动总结
- ✅ **Context Window Management**（上下文窗口管理）：Token计数、自动压缩、多策略支持
- ✅ **Session Management**（会话管理）：持久化、断点恢复、跨会话记忆
- ✅ **Quality Metrics**（质量指标）：多维度质量评分、优化建议
- ✅ **Cross-session Memory Sharing**（跨会话共享）：Private/Shared/Global作用域
- ✅ **User Preference Learning**（用户偏好学习）：自动提取、持久化存储

#### 1.3.4 生产级性能与可靠性

- **高性能**：36.21 ns/op的Middleware Stack性能
- **低内存**：80%+的内存占用降低
- **高并发**：支持100+并发Agent
- **可靠性**：80%+测试覆盖率、34,603行核心代码

### 1.4 论文组织

本白皮书的组织结构如下：

- **第2章**介绍AgentSDK的系统架构和核心设计理念
- **第3-4章**详述事件驱动架构和中间件系统
- **第5-7章**深入分析记忆系统、上下文工程和安全隐私
- **第8-9章**介绍多Agent系统和Provider工具生态
- **第10-11章**讨论实现细节和性能评估
- **第12-14章**展示使用场景、相关工作和经验总结
- **第15-16章**展望未来工作并总结全文

---

## 2. System Architecture（系统架构）

### 2.1 总体架构

AgentSDK采用**三层架构设计**，从上到下分为：

```
┌─────────────────────────────────────────────────────────┐
│                      Agent 层                            │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │
│  │  BaseAgent  │  │WorkflowAgent│  │ MultiAgent  │     │
│  └─────────────┘  └─────────────┘  └─────────────┘     │
│         │                 │                 │            │
│         └─────────────────┴─────────────────┘            │
│                           │                               │
├───────────────────────────┼───────────────────────────────┤
│                      Backend 层                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │
│  │ StateBackend│  │StoreBackend │  │ FSBackend   │     │
│  └─────────────┘  └─────────────┘  └─────────────┘     │
│         │                 │                 │            │
│         └─────────────────┴─────────────────┘            │
│                           │                               │
├───────────────────────────┼───────────────────────────────┤
│                    基础设施层                             │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │
│  │ Sandbox  │  │ Scheduler│  │Permission│  │Telemetry│ │
│  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │
└─────────────────────────────────────────────────────────┘
```

#### 2.1.1 Agent层

**职责**：实现Agent的核心逻辑，包括消息处理、工具调用、记忆管理等。

**主要组件**：

- **BaseAgent**：单Agent实现，处理LLM交互和工具执行
- **WorkflowAgent**：工作流Agent（Parallel/Sequential/Loop）
- **MultiAgent**：多Agent协作（AgentPool/Room）

**核心流程**：

```go
// 简化的Agent运行流程
func (a *BaseAgent) Run(ctx context.Context, input string) iter.Seq2[Event, error] {
    return func(yield func(Event, error) bool) {
        // 1. 加载Working Memory到System Prompt
        memories := a.loadWorkingMemories(ctx)

        // 2. 构建消息列表
        messages := a.buildMessages(input, memories)

        // 3. LLM调用（流式）
        stream := a.provider.Chat(ctx, messages)

        // 4. 处理响应（工具调用、文本输出）
        for event := range stream {
            if !yield(event, nil) {
                return
            }
        }
    }
}
```

#### 2.1.2 Backend层

**职责**：管理Agent的状态和数据存储，提供统一的数据访问接口。

**Backend Protocol接口**：

```go
type Backend interface {
    // Read operations
    Read(ctx context.Context, path string) ([]byte, error)
    List(ctx context.Context, path string) ([]Entry, error)

    // Write operations
    Write(ctx context.Context, path string, data []byte) error
    Delete(ctx context.Context, path string) error

    // Lifecycle
    Close() error
}
```

**四种Backend实现**：

1. **StateBackend**：基于内存的临时存储
   - 用途：测试、临时会话
   - 特点：快速、不持久化

2. **StoreBackend**：基于数据库的持久化存储
   - 用途：生产环境、需要持久化的会话
   - 支持：PostgreSQL、MySQL 8.0+

3. **FilesystemBackend**：真实文件系统操作
   - 用途：文件操作相关任务
   - 特点：直接操作物理文件

4. **CompositeBackend**：路由组合多个Backend
   - 用途：混合存储策略
   - 示例：`/memory/*` → StoreBackend，`/temp/*` → StateBackend

#### 2.1.3 基础设施层

**职责**：提供跨层级的基础服务和能力。

**核心组件**：

1. **Sandbox**：代码执行隔离
   - LocalSandbox（Docker）
   - AliyunSandbox（阿里云AgentBay）
   - VolcengineSandbox（火山引擎）

2. **Scheduler**：任务调度
   - 长时运行任务管理
   - 取消和超时控制

3. **Permission**：权限管理
   - 工具级权限（allow/deny/ask）
   - 动态审批流程

4. **Telemetry**：可观测性
   - OpenTelemetry集成
   - 分布式追踪
   - 指标收集

### 2.2 组件交互流程

完整的请求处理流程：

```
User Input
    ↓
┌─────────────────────────────────────────────────┐
│ Agent.Run()                                     │
│  ↓                                              │
│  1. Middleware Stack (Before)                  │
│     ├─ FilesystemMiddleware                    │
│     ├─ SubAgentMiddleware                      │
│     ├─ SummarizationMiddleware                 │
│     └─ PIIRedactionMiddleware                  │
│  ↓                                              │
│  2. Load Working Memory                        │
│     └─ Backend.Read("/memory/working/*")       │
│  ↓                                              │
│  3. LLM Provider.Chat()                        │
│     └─ Stream Events → Progress Channel        │
│  ↓                                              │
│  4. Tool Execution                              │
│     ├─ Permission Check                         │
│     ├─ Sandbox Execution (if needed)           │
│     └─ Result → Backend.Write()                │
│  ↓                                              │
│  5. Middleware Stack (After)                   │
│     └─ Response transformation                 │
│  ↓                                              │
│  6. Emit Events                                 │
│     ├─ Progress Channel (text, tools)          │
│     ├─ Control Channel (approvals)             │
│     └─ Monitor Channel (audit)                 │
└─────────────────────────────────────────────────┘
    ↓
Response to User
```

### 2.3 核心设计模式

#### 2.3.1 Protocol + Factory 模式

**问题**：避免硬编码依赖，实现延迟初始化和依赖注入。

**解决方案**：

```go
// Protocol: 定义接口
type Backend interface {
    Read(ctx context.Context, path string) ([]byte, error)
    Write(ctx context.Context, path string, data []byte) error
}

// Factory: 延迟初始化
type BackendFactory interface {
    CreateBackend(config BackendConfig) (Backend, error)
}

// 使用示例
type Agent struct {
    backendFactory BackendFactory
    backend        Backend  // lazy initialized
}

func (a *Agent) getBackend() Backend {
    if a.backend == nil {
        a.backend = a.backendFactory.CreateBackend(a.config)
    }
    return a.backend
}
```

**优势**：
- **松耦合**：Agent不依赖具体Backend实现
- **可测试**：轻松注入Mock实现
- **灵活配置**：运行时决定使用哪个Backend

#### 2.3.2 洋葱模型（Onion Model）

**问题**：如何实现灵活的请求/响应拦截？

**解决方案**：

```go
type Middleware interface {
    Name() string
    Priority() int
    Before(ctx context.Context, req *Request) (*Request, error)
    After(ctx context.Context, resp *Response) (*Response, error)
}

// 执行流程
func executeMiddlewareStack(req *Request) *Response {
    // Before phase (outer → inner)
    for _, mw := range sortedMiddlewares {
        req = mw.Before(ctx, req)
    }

    // Core execution
    resp := coreHandler(req)

    // After phase (inner → outer)
    for i := len(sortedMiddlewares) - 1; i >= 0; i-- {
        resp = sortedMiddlewares[i].After(ctx, resp)
    }

    return resp
}
```

#### 2.3.3 观察者模式（Event Channels）

**问题**：如何实现解耦的事件通知？

**解决方案**：三通道设计

```go
type Agent struct {
    progressChan chan Event   // 实时进度
    controlChan  chan Event   // 控制信号
    monitorChan  chan Event   // 审计监控
}

// 订阅者
func (subscriber *UIComponent) Subscribe(agent *Agent) {
    go func() {
        for event := range agent.progressChan {
            subscriber.OnProgress(event)
        }
    }()
}
```

#### 2.3.4 策略模式（Compression Strategies）

**问题**：如何支持多种可替换的算法？

**解决方案**：

```go
type CompressionStrategy interface {
    Compress(messages []Message) []Message
}

// 具体策略
type SlidingWindowStrategy struct { windowSize int }
type PriorityBasedStrategy struct { calculator PriorityCalculator }
type TokenBasedStrategy struct { targetUsage float64 }

// 上下文
type ContextWindowManager struct {
    strategy CompressionStrategy
}

func (m *ContextWindowManager) SetStrategy(s CompressionStrategy) {
    m.strategy = s
}
```

### 2.4 数据流分析

#### 2.4.1 Write Path（写路径）

```
User Input
    ↓
Agent.Run()
    ↓
Middleware.Before()  ← PII检测和脱敏
    ↓
Working Memory Load  ← Backend.Read()
    ↓
LLM Provider.Chat()
    ↓
Tool Execution
    ↓
Result Store         ← Backend.Write()
    ↓
Middleware.After()   ← 响应处理
    ↓
Event Emission       ← Channels
```

#### 2.4.2 Read Path（读路径）

```
Query Request
    ↓
Semantic Memory Search
    ↓
Vector Similarity
    ↓
Quality Scoring
    ↓
Provenance Check
    ↓
Ranked Results
    ↓
Context Building
```

### 2.5 架构优势

AgentSDK的架构设计带来以下优势：

1. **清晰的关注点分离**：每一层专注自己的职责
2. **高度可测试**：每个组件可以独立测试
3. **灵活的扩展性**：通过Protocol和Factory轻松扩展
4. **生产就绪**：完整的错误处理、审计、可观测性
5. **高性能**：零拷贝、流式处理、连接池优化

---

## 3. Event-Driven Architecture（事件驱动架构）

### 3.1 三通道设计动机

传统的Agent框架通常采用单一的消息通道，这导致了几个问题：

1. **实时性差**：用户只能在Agent完成后看到结果，无法感知中间过程
2. **可控性弱**：无法中途暂停或介入Agent执行
3. **可观测性缺失**：难以追踪Agent的决策过程和工具调用

AgentSDK创新性地提出**三通道架构**，彻底分离不同类型的事件：

```
┌──────────────────────────────────────────────┐
│              Agent Runtime                   │
│  ┌────────────────────────────────────────┐ │
│  │  Progress Channel (实时进度)           │ │
│  │  ├─ TextDelta (文本流)                 │ │
│  │  ├─ ToolCall (工具调用)                │ │
│  │  ├─ ToolResult (工具结果)              │ │
│  │  └─ Thinking (思考过程)                │ │
│  └────────────────────────────────────────┘ │
│  ┌────────────────────────────────────────┐ │
│  │  Control Channel (控制交互)            │ │
│  │  ├─ Approval Request (审批请求)        │ │
│  │  ├─ User Input (用户输入)              │ │
│  │  └─ Pause/Continue (暂停/继续)         │ │
│  └────────────────────────────────────────┘ │
│  ┌────────────────────────────────────────┐ │
│  │  Monitor Channel (监控审计)            │ │
│  │  ├─ AuditEvent (审计事件)              │ │
│  │  ├─ MetricEvent (性能指标)             │ │
│  │  └─ ErrorEvent (错误追踪)              │ │
│  └────────────────────────────────────────┘ │
└──────────────────────────────────────────────┘
```

### 3.2 Progress Channel（进度通道）

**用途**：向用户实时展示Agent的执行进度。

**事件类型**：

```go
type ProgressEvent struct {
    Type      EventType        // TextDelta, ToolCall, ToolResult, etc.
    Timestamp time.Time
    Data      interface{}
}

// 文本增量事件
type TextDelta struct {
    Delta string    // 新增的文本片段
    Index int       // 在完整文本中的位置
}

// 工具调用事件
type ToolCall struct {
    ID        string
    Name      string
    Arguments map[string]interface{}
}

// 工具结果事件
type ToolResult struct {
    ID     string
    Result interface{}
    Error  error
}
```

**使用示例**：

```go
// 订阅Progress事件
for event := range agent.ProgressChannel() {
    switch e := event.Data.(type) {
    case TextDelta:
        // 流式显示文本
        ui.AppendText(e.Delta)

    case ToolCall:
        // 显示工具调用
        ui.ShowToolExecution(e.Name, e.Arguments)

    case ToolResult:
        // 显示工具结果
        ui.ShowToolResult(e.ID, e.Result)
    }
}
```

### 3.3 Control Channel（控制通道）

**用途**：实现Agent与用户的双向交互。

**核心场景**：

1. **工具审批**：危险操作需要人工批准
2. **用户输入**：Agent请求额外信息
3. **暂停/继续**：用户控制Agent执行

**示例**：工具审批流程

```go
// Agent请求审批
approvalRequest := ApprovalEvent{
    ToolName:   "delete_file",
    Arguments:  map[string]interface{}{"path": "/important/data.txt"},
    Reason:     "This operation will delete important file",
}
agent.controlChan <- approvalRequest

// 用户响应
response := <-agent.controlChan
if response.Approved {
    // 执行操作
    result := executeTool(approvalRequest)
} else {
    // 拒绝操作
    return errors.New("user denied the operation")
}
```

### 3.4 Monitor Channel（监控通道）

**用途**：治理、审计和性能监控。

**审计事件**：

```go
type AuditEvent struct {
    EventID    string
    Type       AuditType    // ToolExecution, MemoryAccess, DataExport
    Actor      string       // Agent ID
    Resource   string       // 操作的资源
    Action     string       // 操作类型
    Timestamp  time.Time
    Metadata   map[string]interface{}
}
```

**集成OpenTelemetry**：

```go
func (a *Agent) emitAuditEvent(event AuditEvent) {
    // 发送到Monitor通道
    a.monitorChan <- event

    // 同时记录到OpenTelemetry
    span := trace.SpanFromContext(ctx)
    span.AddEvent("audit", trace.WithAttributes(
        attribute.String("type", string(event.Type)),
        attribute.String("resource", event.Resource),
    ))
}
```

### 3.5 流式处理实现

AgentSDK使用Go 1.23的`iter.Seq2`实现高效的流式处理：

```go
// 传统方式：需要分配大量内存
func TraditionalRun(input string) ([]Event, error) {
    events := []Event{}  // 需要存储所有事件
    for {
        event := generateEvent()
        events = append(events, event)  // 内存不断增长
    }
    return events, nil
}

// iter.Seq2方式：惰性生成，零内存开销
func (a *Agent) Run(ctx context.Context, input string) iter.Seq2[Event, error] {
    return func(yield func(Event, error) bool) {
        for {
            event := a.generateEvent()
            if !yield(event, nil) {
                return  // 消费者停止，立即释放资源
            }
        }
    }
}

// 消费者可以随时停止
for event, err := range agent.Run(ctx, input) {
    if shouldStop(event) {
        break  // 停止迭代，Agent立即停止
    }
    process(event)
}
```

**性能优势**：

- **内存占用降低80%+**：不需要缓存所有事件
- **实时性**：事件立即传递给消费者
- **背压控制**：消费者处理速度自然限制生产速度

### 3.6 事件过滤与订阅

**问题**：不同的订阅者可能只关心特定类型的事件。

**解决方案**：基于类型的事件过滤

```go
type EventFilter struct {
    Types    []EventType      // 感兴趣的事件类型
    Channels []ChannelType    // 订阅的通道
}

func (a *Agent) Subscribe(filter EventFilter) <-chan Event {
    ch := make(chan Event, 100)

    go func() {
        defer close(ch)

        for event := range a.allEvents() {
            if filter.Matches(event) {
                ch <- event
            }
        }
    }()

    return ch
}

// 使用示例
uiEvents := agent.Subscribe(EventFilter{
    Types:    []EventType{TextDelta, ToolCall},
    Channels: []ChannelType{ProgressChannel},
})

auditEvents := agent.Subscribe(EventFilter{
    Types:    []EventType{AuditEvent},
    Channels: []ChannelType{MonitorChannel},
})
```

## 4. Middleware System（中间件系统）

### 4.1 洋葱模型设计

AgentSDK借鉴了Web框架（如Koa.js、Express.js）的中间件设计理念，实现了**洋葱模型（Onion Model）**。这种设计允许在Agent执行的Before和After阶段插入自定义逻辑。

#### 4.1.1 核心概念

```
┌─────────────────────────────────────────────┐
│          Request Flow (Before)              │
│                                             │
│  User Input                                 │
│      ↓                                      │
│  ┌──────────────────────────┐              │
│  │ Middleware 1 (Before)    │              │
│  │  ┌────────────────────┐  │              │
│  │  │ Middleware 2       │  │              │
│  │  │  ┌──────────────┐  │  │              │
│  │  │  │ Middleware 3 │  │  │              │
│  │  │  │  ┌────────┐  │  │  │              │
│  │  │  │  │  Core  │  │  │  │              │
│  │  │  │  │ Agent  │  │  │  │              │
│  │  │  │  └────────┘  │  │  │              │
│  │  │  │ Middleware 3 │  │  │              │
│  │  │  │   (After)    │  │  │              │
│  │  │  └──────────────┘  │  │              │
│  │  │ Middleware 2       │  │              │
│  │  │   (After)          │  │              │
│  │  └────────────────────┘  │              │
│  │ Middleware 1 (After)     │              │
│  └──────────────────────────┘              │
│      ↓                                      │
│  Response to User                           │
└─────────────────────────────────────────────┘
```

**执行流程**：

1. **Before阶段**（外→内）：
   - 优先级从高到低执行
   - 可以修改请求（如PII脱敏）
   - 可以添加上下文信息
   - 可以短路执行（返回错误或直接返回）

2. **Core执行**：Agent核心逻辑运行

3. **After阶段**（内→外）：
   - 优先级从低到高执行
   - 可以修改响应
   - 可以添加额外数据
   - 可以记录审计日志

#### 4.1.2 优先级排序

中间件通过优先级控制执行顺序：

```go
type MiddlewarePriority int

const (
    PriorityHighest  MiddlewarePriority = 1000
    PriorityHigh     MiddlewarePriority = 500
    PriorityNormal   MiddlewarePriority = 100
    PriorityLow      MiddlewarePriority = 50
    PriorityLowest   MiddlewarePriority = 1
)

// 内置中间件优先级
const (
    PIIRedactionPriority     = PriorityHighest  // 最先执行，最后恢复
    FilesystemPriority       = PriorityHigh
    SubAgentPriority         = PriorityNormal
    SummarizationPriority    = PriorityLow
)
```

**设计原则**：
- **安全中间件优先级最高**（如PII脱敏）
- **资源中间件次之**（如文件系统）
- **功能增强中间件居中**（如SubAgent）
- **优化中间件优先级最低**（如自动总结）

### 4.2 中间件接口

#### 4.2.1 核心Protocol

```go
// Middleware 中间件接口
type Middleware interface {
    // 中间件名称（用于日志和调试）
    Name() string

    // 执行优先级（数字越大优先级越高）
    Priority() int

    // Before在Agent执行前调用
    // 返回修改后的请求或错误
    Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error)

    // After在Agent执行后调用
    // 返回修改后的响应或错误
    After(ctx context.Context, resp *AgentResponse) (*AgentResponse, error)
}

// AgentRequest 请求上下文
type AgentRequest struct {
    Input      string                 // 用户输入
    Messages   []Message              // 消息历史
    Metadata   map[string]interface{} // 元数据
    Context    context.Context        // Go context
}

// AgentResponse 响应上下文
type AgentResponse struct {
    Output     string                 // Agent输出
    Events     []Event                // 事件列表
    ToolCalls  []ToolCall             // 工具调用
    Metadata   map[string]interface{} // 元数据
    Error      error                  // 错误信息
}
```

#### 4.2.2 中间件管理器

```go
type MiddlewareManager struct {
    middlewares []Middleware
    mu          sync.RWMutex
}

// Register 注册中间件
func (mm *MiddlewareManager) Register(mw Middleware) {
    mm.mu.Lock()
    defer mm.mu.Unlock()

    mm.middlewares = append(mm.middlewares, mw)

    // 按优先级排序（降序）
    sort.Slice(mm.middlewares, func(i, j int) bool {
        return mm.middlewares[i].Priority() > mm.middlewares[j].Priority()
    })
}

// Execute 执行中间件栈
func (mm *MiddlewareManager) Execute(
    ctx context.Context,
    req *AgentRequest,
    handler func(*AgentRequest) (*AgentResponse, error),
) (*AgentResponse, error) {
    mm.mu.RLock()
    defer mm.mu.RUnlock()

    // Before阶段
    for _, mw := range mm.middlewares {
        modifiedReq, err := mw.Before(ctx, req)
        if err != nil {
            return nil, fmt.Errorf("middleware %s before failed: %w", mw.Name(), err)
        }
        req = modifiedReq
    }

    // 核心执行
    resp, err := handler(req)
    if err != nil {
        return nil, err
    }

    // After阶段（逆序）
    for i := len(mm.middlewares) - 1; i >= 0; i-- {
        mw := mm.middlewares[i]
        modifiedResp, err := mw.After(ctx, resp)
        if err != nil {
            return nil, fmt.Errorf("middleware %s after failed: %w", mw.Name(), err)
        }
        resp = modifiedResp
    }

    return resp, nil
}
```

**性能指标**：

```
BenchmarkMiddlewareStack-8   33,000,000   36.21 ns/op   0 B/op   0 allocs/op
```

### 4.3 内置中间件

AgentSDK提供了四个生产级的内置中间件：

#### 4.3.1 FilesystemMiddleware（文件系统中间件）

**用途**：拦截文件操作，路由到Backend或真实文件系统。

**核心逻辑**：

```go
type FilesystemMiddleware struct {
    backend Backend
    realFS  bool  // 是否允许真实文件系统操作
}

func (m *FilesystemMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    // 拦截文件操作工具调用
    for _, toolCall := range req.ToolCalls {
        if isFileOperation(toolCall.Name) {
            path := toolCall.Arguments["path"].(string)

            if m.realFS {
                // 真实文件系统操作
                result := m.executeRealFileOp(toolCall)
                req.Metadata["file_result"] = result
            } else {
                // 虚拟文件系统（Backend）
                result := m.executeBackendOp(ctx, toolCall)
                req.Metadata["file_result"] = result
            }

            // 标记为已处理，跳过实际执行
            toolCall.Handled = true
        }
    }
    return req, nil
}
```

**安全特性**：
- 路径验证（防止目录遍历攻击）
- 权限检查
- 操作审计

#### 4.3.2 SubAgentMiddleware（子Agent中间件）

**用途**：识别需要委派的子任务，创建并执行专用子Agent。

**委派策略**：

```go
type SubAgentMiddleware struct {
    agentPool  *AgentPool
    dispatcher *TaskDispatcher
}

func (m *SubAgentMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    // 分析任务复杂度
    tasks := m.dispatcher.AnalyzeTasks(req.Input)

    for _, task := range tasks {
        if task.ShouldDelegate() {
            // 创建专用子Agent
            subAgent := m.agentPool.GetAgent(task.Type)

            // 并行执行
            result, err := subAgent.Run(ctx, task.Input)
            if err != nil {
                return nil, err
            }

            // 将结果注入到请求中
            req.Metadata["subtask_results"] = append(
                req.Metadata["subtask_results"].([]SubtaskResult),
                SubtaskResult{Task: task, Result: result},
            )
        }
    }

    return req, nil
}
```

**适用场景**：
- 代码生成 → CodeAgent
- 数据分析 → DataAgent
- 文档编写 → WriterAgent

#### 4.3.3 SummarizationMiddleware（自动总结中间件）

**用途**：长对话自动压缩，防止Token溢出。

**压缩策略**：

```go
type SummarizationMiddleware struct {
    compressor    *SessionCompressor
    tokenCounter  *TokenCounter
    maxTokens     int
    triggerRatio  float64  // 触发压缩的比例（如0.8 = 80%）
}

func (m *SummarizationMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    // 计算当前Token使用量
    currentTokens := m.tokenCounter.CountMessages(req.Messages)
    usageRatio := float64(currentTokens) / float64(m.maxTokens)

    if usageRatio >= m.triggerRatio {
        // 触发压缩
        compressed, err := m.compressor.CompressMessages(ctx, req.Messages)
        if err != nil {
            return nil, err
        }

        req.Messages = compressed
        req.Metadata["compression_applied"] = true
        req.Metadata["tokens_saved"] = currentTokens - m.tokenCounter.CountMessages(compressed)
    }

    return req, nil
}
```

**压缩级别**：
- **Light（轻度）**：仅删除重复内容，保留原始消息
- **Moderate（中度）**：LLM生成简洁总结
- **Aggressive（激进）**：仅保留关键信息

#### 4.3.4 PIIRedactionMiddleware（PII脱敏中间件）

**用途**：自动检测并脱敏个人敏感信息（PII）。

**检测引擎**：

```go
type PIIRedactionMiddleware struct {
    detector PIIDetector
    strategy RedactionStrategy
}

// 支持的PII类型
const (
    PIIEmail           = "email"            // 邮箱
    PIIPhone           = "phone"            // 电话
    PIICreditCard      = "credit_card"      // 信用卡
    PIIIDCard          = "id_card"          // 身份证
    PIIBankAccount     = "bank_account"     // 银行账号
    PIIIPAddress       = "ip_address"       // IP地址
    PIIPassport        = "passport"         // 护照
    PIISSNumber        = "ssn"              // 社保号
    PIIAddress         = "address"          // 地址
    PIIName            = "name"             // 姓名
)

func (m *PIIRedactionMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    // 扫描输入
    findings := m.detector.Detect(req.Input)

    if len(findings) > 0 {
        // 应用脱敏策略
        redacted := m.strategy.Redact(req.Input, findings)
        req.Input = redacted

        // 记录脱敏信息（用于After阶段恢复）
        req.Metadata["pii_redactions"] = findings
    }

    return req, nil
}

func (m *PIIRedactionMiddleware) After(ctx context.Context, resp *AgentResponse) (*AgentResponse, error) {
    // 可选：恢复脱敏内容（仅在安全环境）
    if findings, ok := resp.Metadata["pii_redactions"].([]PIIFinding); ok {
        resp.Output = m.strategy.Restore(resp.Output, findings)
    }

    return resp, nil
}
```

**脱敏策略**：

1. **MASK（掩码）**：`john.doe@example.com` → `j***@e***.com`
2. **REPLACE（替换）**：`john.doe@example.com` → `[EMAIL]`
3. **HASH（哈希）**：`john.doe@example.com` → `<EMAIL:3a7bd3e2>`
4. **REMOVE（删除）**：完全移除PII内容

### 4.4 自定义中间件开发

#### 4.4.1 开发指南

**示例：RateLimitMiddleware**

```go
type RateLimitMiddleware struct {
    limiter *rate.Limiter
    userLimiters sync.Map  // userID -> *rate.Limiter
}

func NewRateLimitMiddleware(rps int, burst int) *RateLimitMiddleware {
    return &RateLimitMiddleware{
        limiter: rate.NewLimiter(rate.Limit(rps), burst),
    }
}

func (m *RateLimitMiddleware) Name() string {
    return "RateLimitMiddleware"
}

func (m *RateLimitMiddleware) Priority() int {
    return middleware.PriorityHigh
}

func (m *RateLimitMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    userID := req.Metadata["user_id"].(string)

    // 获取或创建用户限流器
    limiterVal, _ := m.userLimiters.LoadOrStore(userID, rate.NewLimiter(rate.Limit(10), 20))
    userLimiter := limiterVal.(*rate.Limiter)

    // 检查限流
    if !userLimiter.Allow() {
        return nil, errors.New("rate limit exceeded")
    }

    return req, nil
}

func (m *RateLimitMiddleware) After(ctx context.Context, resp *AgentResponse) (*AgentResponse, error) {
    // 不需要After处理
    return resp, nil
}
```

#### 4.4.2 最佳实践

**1. 单一职责原则**

每个中间件只做一件事，保持代码简洁：

```go
// ✅ Good: 职责单一
type LoggingMiddleware struct { ... }
type AuthMiddleware struct { ... }
type CachingMiddleware struct { ... }

// ❌ Bad: 职责混杂
type SuperMiddleware struct {
    logger  *Logger
    auth    *Authenticator
    cache   *Cache
}
```

**2. 错误处理**

明确区分可恢复错误和不可恢复错误：

```go
func (m *MyMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    result, err := m.doSomething(req)
    if err != nil {
        if isRecoverable(err) {
            // 记录警告，继续执行
            log.Warn("middleware warning", "error", err)
            return req, nil
        } else {
            // 不可恢复，返回错误
            return nil, fmt.Errorf("critical error in %s: %w", m.Name(), err)
        }
    }

    req.Metadata["my_result"] = result
    return req, nil
}
```

**3. 资源清理**

确保在After阶段清理资源：

```go
func (m *ConnectionPoolMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    conn := m.pool.Get()
    req.Metadata["db_conn"] = conn
    return req, nil
}

func (m *ConnectionPoolMiddleware) After(ctx context.Context, resp *AgentResponse) (*AgentResponse, error) {
    if conn, ok := resp.Metadata["db_conn"].(DBConnection); ok {
        m.pool.Put(conn)  // 归还连接
        delete(resp.Metadata, "db_conn")
    }
    return resp, nil
}
```

**4. Context传播**

正确使用Go context：

```go
func (m *TracingMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    // 创建新的span
    ctx, span := tracer.Start(ctx, "middleware."+m.Name())

    // 将新context注入请求
    req.Context = ctx
    req.Metadata["trace_span"] = span

    return req, nil
}

func (m *TracingMiddleware) After(ctx context.Context, resp *AgentResponse) (*AgentResponse, error) {
    // 结束span
    if span, ok := resp.Metadata["trace_span"].(trace.Span); ok {
        span.End()
        delete(resp.Metadata, "trace_span")
    }
    return resp, nil
}
```

### 4.5 性能优化

#### 4.5.1 性能指标

中间件栈的性能至关重要，AgentSDK的优化成果：

```
# 无中间件
BenchmarkAgent_NoMiddleware-8     10,000   105,234 ns/op   4,096 B/op   42 allocs/op

# 1个中间件
BenchmarkAgent_1Middleware-8      9,500    105,271 ns/op   4,128 B/op   43 allocs/op

# 4个中间件
BenchmarkAgent_4Middlewares-8     9,200    105,379 ns/op   4,224 B/op   46 allocs/op

# 中间件栈开销
BenchmarkMiddlewareStack-8        33,000,000   36.21 ns/op   0 B/op   0 allocs/op
```

**结论**：中间件栈的开销约为36.21 ns，对整体性能影响小于0.14%。

#### 4.5.2 优化技巧

**1. 避免不必要的分配**

```go
// ❌ Bad: 每次调用都分配新map
func (m *MyMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    req.Metadata = make(map[string]interface{})  // 新分配
    req.Metadata["key"] = "value"
    return req, nil
}

// ✅ Good: 复用现有map
func (m *MyMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    if req.Metadata == nil {
        req.Metadata = make(map[string]interface{}, 4)  // 预分配容量
    }
    req.Metadata["key"] = "value"
    return req, nil
}
```

**2. 使用对象池**

```go
var bufferPool = sync.Pool{
    New: func() interface{} {
        return new(bytes.Buffer)
    },
}

func (m *MyMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    buf := bufferPool.Get().(*bytes.Buffer)
    defer func() {
        buf.Reset()
        bufferPool.Put(buf)
    }()

    // 使用buffer
    buf.WriteString(req.Input)
    // ...

    return req, nil
}
```

**3. 惰性初始化**

```go
type ExpensiveMiddleware struct {
    expensiveResource *ExpensiveResource
    once              sync.Once
}

func (m *ExpensiveMiddleware) getResource() *ExpensiveResource {
    m.once.Do(func() {
        m.expensiveResource = NewExpensiveResource()
    })
    return m.expensiveResource
}

func (m *ExpensiveMiddleware) Before(ctx context.Context, req *AgentRequest) (*AgentRequest, error) {
    resource := m.getResource()  // 首次调用时才初始化
    // ...
    return req, nil
}
```

**4. 并行执行**

对于独立的中间件操作，可以并行执行：

```go
func (mm *MiddlewareManager) ExecuteParallel(ctx context.Context, req *AgentRequest) error {
    var wg sync.WaitGroup
    errChan := make(chan error, len(mm.middlewares))

    for _, mw := range mm.middlewares {
        wg.Add(1)
        go func(m Middleware) {
            defer wg.Done()
            if _, err := m.Before(ctx, req); err != nil {
                errChan <- err
            }
        }(mw)
    }

    wg.Wait()
    close(errChan)

    // 检查错误
    for err := range errChan {
        if err != nil {
            return err
        }
    }

    return nil
}
```

## 5. Memory System（记忆系统）

AgentSDK实现了完整的三层记忆系统，并完整支持Google Context Engineering白皮书的所有核心功能。

### 5.1 三层记忆架构

```
┌─────────────────────────────────────────────────┐
│             Memory System                       │
│                                                 │
│  ┌───────────────────────────────────────────┐ │
│  │  Layer 1: Text Memory (文本记忆)          │ │
│  │  - 原始消息存储                            │ │
│  │  - 完整对话历史                            │ │
│  │  - 不压缩、不总结                          │ │
│  └───────────────────────────────────────────┘ │
│                    ↓                            │
│  ┌───────────────────────────────────────────┐ │
│  │  Layer 2: Working Memory (工作记忆)       │ │
│  │  - 当前会话活跃信息                        │ │
│  │  - 压缩后的历史                            │ │
│  │  - 注入到System Prompt                    │ │
│  └───────────────────────────────────────────┘ │
│                    ↓                            │
│  ┌───────────────────────────────────────────┐ │
│  │  Layer 3: Semantic Memory (语义记忆)      │ │
│  │  - 向量化存储                              │ │
│  │  - 语义检索                                │ │
│  │  - 长期知识库                              │ │
│  └───────────────────────────────────────────┘ │
└─────────────────────────────────────────────────┘
```

#### 5.1.1 Text Memory（文本记忆）

**用途**：保存完整的原始对话历史，用于审计和回溯。

**特点**：
- **不压缩**：保留所有细节
- **持久化**：存储到Backend
- **可追溯**：完整的时间线

**接口**：

```go
type TextMemory interface {
    // 添加消息
    Add(ctx context.Context, message Message) error

    // 获取所有消息
    GetAll(ctx context.Context) ([]Message, error)

    // 获取时间范围内的消息
    GetRange(ctx context.Context, start, end time.Time) ([]Message, error)

    // 清除所有消息
    Clear(ctx context.Context) error
}
```

#### 5.1.2 Working Memory（工作记忆）

**用途**：维护当前会话的活跃信息，注入到LLM的System Prompt。

**关键功能**：
- **Token预算管理**：控制Working Memory总大小
- **智能压缩**：自动压缩旧消息
- **优先级排序**：重要信息优先保留

**实现**：

```go
type WorkingMemory struct {
    memories     []*MemoryItem
    tokenBudget  int
    tokenCounter *TokenCounter
    compressor   *MemoryCompressor
}

// MemoryItem 工作记忆项
type MemoryItem struct {
    ID          string
    Content     string
    Priority    float64    // 优先级 (0.0-1.0)
    CreatedAt   time.Time
    LastAccess  time.Time
    AccessCount int
    Metadata    map[string]interface{}
}

// BuildSystemPrompt 构建System Prompt
func (wm *WorkingMemory) BuildSystemPrompt(ctx context.Context) (string, error) {
    // 1. 按优先级排序
    sorted := wm.sortByPriority()

    // 2. 计算Token使用
    currentTokens := 0
    selected := []*MemoryItem{}

    for _, item := range sorted {
        tokens := wm.tokenCounter.Count(item.Content)
        if currentTokens+tokens <= wm.tokenBudget {
            selected = append(selected, item)
            currentTokens += tokens
        } else {
            break
        }
    }

    // 3. 如果超出预算，触发压缩
    if len(selected) < len(sorted) {
        compressed, err := wm.compressor.Compress(ctx, sorted)
        if err != nil {
            return "", err
        }
        selected = compressed
    }

    // 4. 生成System Prompt
    return wm.formatPrompt(selected), nil
}
```

**压缩策略**：

1. **时间衰减**：旧记忆降低优先级
2. **访问频率**：常用记忆提高优先级
3. **相关性**：与当前任务相关的记忆提高优先级

#### 5.1.3 Semantic Memory（语义记忆）

**用途**：长期知识库，基于向量相似度检索。

**核心功能**：

```go
type SemanticMemory struct {
    vectorDB   VectorDatabase
    embedder   Embedder
    provenance *ProvenanceManager  // 溯源管理
    quality    *QualityAnalyzer    // 质量分析
}

// Store 存储记忆
func (sm *SemanticMemory) Store(ctx context.Context, content string, metadata Metadata) error {
    // 1. 生成Embedding
    embedding, err := sm.embedder.Embed(ctx, content)
    if err != nil {
        return err
    }

    // 2. 创建溯源信息
    provenance := sm.provenance.Create(SourceTypeUserProvided, content, metadata)

    // 3. 质量评估
    quality := sm.quality.Analyze(content, provenance)

    // 4. 存储到向量数据库
    return sm.vectorDB.Insert(ctx, VectorRecord{
        ID:         generateID(),
        Embedding:  embedding,
        Content:    content,
        Provenance: provenance,
        Quality:    quality,
        Metadata:   metadata,
    })
}

// Search 语义检索
func (sm *SemanticMemory) Search(ctx context.Context, query string, limit int) ([]*MemoryResult, error) {
    // 1. 生成查询Embedding
    queryEmbedding, err := sm.embedder.Embed(ctx, query)
    if err != nil {
        return nil, err
    }

    // 2. 向量检索
    results, err := sm.vectorDB.Search(ctx, queryEmbedding, limit*2)  // 多取一些候选
    if err != nil {
        return nil, err
    }

    // 3. 质量过滤和重排序
    filtered := sm.quality.Filter(results)
    ranked := sm.quality.Rank(filtered, query)

    // 4. 返回Top-N
    if len(ranked) > limit {
        ranked = ranked[:limit]
    }

    return ranked, nil
}
```

### 5.2 Memory Provenance（内存溯源）

**目标**：追踪每条记忆的来源、置信度和演化历史。

#### 5.2.1 四种来源类型

```go
type SourceType string

const (
    SourceTypeUserProvided   SourceType = "user_provided"     // 用户明确提供
    SourceTypeAgentObserved  SourceType = "agent_observed"    // Agent观察推断
    SourceTypeExternalFetch  SourceType = "external_fetch"    // 外部API获取
    SourceTypeDerived        SourceType = "derived"           // 从其他记忆派生
)

type MemoryProvenance struct {
    ID              string
    SourceType      SourceType
    SourceID        string              // 来源标识（如API URL）
    OriginalContent string              // 原始内容
    Confidence      float64             // 置信度 (0.0-1.0)
    Timestamp       time.Time
    Lineage         []string            // 谱系链（父记忆ID列表）
    Metadata        map[string]string
}
```

#### 5.2.2 置信度计算

置信度受多个因素影响：

```go
type ConfidenceCalculator struct {
    timeDecay     float64  // 时间衰减因子
    sourceWeights map[SourceType]float64
}

// Calculate 计算置信度
func (cc *ConfidenceCalculator) Calculate(prov *MemoryProvenance) float64 {
    // 1. 基础置信度（基于来源类型）
    baseConfidence := cc.sourceWeights[prov.SourceType]

    // 2. 时间衰减
    daysSince := time.Since(prov.Timestamp).Hours() / 24
    decayFactor := math.Exp(-cc.timeDecay * daysSince)

    // 3. 综合计算
    confidence := baseConfidence * decayFactor

    return math.Max(0.0, math.Min(1.0, confidence))
}

// 默认权重
var DefaultSourceWeights = map[SourceType]float64{
    SourceTypeUserProvided:  1.0,   // 最高置信度
    SourceTypeExternalFetch: 0.9,   // API数据较可靠
    SourceTypeAgentObserved: 0.7,   // Agent推断中等可靠
    SourceTypeDerived:       0.6,   // 派生数据最低
}
```

**置信度提升机制**：

```go
// BoostConfidence 通过多源佐证提升置信度
func (sm *SemanticMemory) BoostConfidence(ctx context.Context, memoryID string, evidence []string) error {
    memory, err := sm.Get(ctx, memoryID)
    if err != nil {
        return err
    }

    // 计算佐证权重
    boostFactor := 1.0 + (0.1 * float64(len(evidence)))  // 每个佐证提升10%
    newConfidence := memory.Provenance.Confidence * boostFactor

    // 更新置信度（不超过1.0）
    memory.Provenance.Confidence = math.Min(1.0, newConfidence)

    return sm.Update(ctx, memory)
}
```

#### 5.2.3 谱系管理（Lineage）

**用途**：追踪记忆的演化历史，支持级联删除。

```go
type LineageManager struct {
    graph *MemoryGraph
}

// AddChild 添加派生记忆
func (lm *LineageManager) AddChild(parentID, childID string) error {
    return lm.graph.AddEdge(parentID, childID)
}

// GetLineage 获取完整谱系链
func (lm *LineageManager) GetLineage(memoryID string) ([]string, error) {
    return lm.graph.GetAncestors(memoryID)
}

// CascadeDelete 级联删除
func (lm *LineageManager) CascadeDelete(ctx context.Context, memoryID string) error {
    // 1. 获取所有后代
    descendants, err := lm.graph.GetDescendants(memoryID)
    if err != nil {
        return err
    }

    // 2. 删除后代（从叶子节点开始）
    for i := len(descendants) - 1; i >= 0; i-- {
        if err := lm.deleteMemory(ctx, descendants[i]); err != nil {
            return err
        }
    }

    // 3. 删除自身
    return lm.deleteMemory(ctx, memoryID)
}
```

### 5.3 Memory Consolidation（内存合并）

**目标**：自动检测并合并冗余、冲突的记忆，减少噪声。

#### 5.3.1 三种合并策略

```go
type ConsolidationStrategy int

const (
    StrategyRedundancy ConsolidationStrategy = 0  // 冗余合并
    StrategyConflict   ConsolidationStrategy = 1  // 冲突解决
    StrategySummary    ConsolidationStrategy = 2  // 总结压缩
)
```

#### 5.3.2 冗余检测与合并

```go
type RedundancyDetector struct {
    similarityThreshold float64  // 相似度阈值（如0.9）
    embedder            Embedder
}

// Detect 检测冗余记忆
func (rd *RedundancyDetector) Detect(ctx context.Context, memories []*Memory) ([][]*Memory, error) {
    clusters := [][]*Memory{}

    for _, mem := range memories {
        // 查找相似记忆
        similar := rd.findSimilar(ctx, mem, memories)

        if len(similar) > 0 {
            clusters = append(clusters, similar)
        }
    }

    return clusters, nil
}

// Merge 合并冗余记忆
func (rd *RedundancyDetector) Merge(ctx context.Context, cluster []*Memory) (*Memory, error) {
    // 1. 选择最高质量的记忆作为基础
    base := rd.selectBest(cluster)

    // 2. 合并元数据
    mergedMetadata := rd.mergeMetadata(cluster)

    // 3. 综合置信度（取最高）
    maxConfidence := 0.0
    for _, mem := range cluster {
        if mem.Provenance.Confidence > maxConfidence {
            maxConfidence = mem.Provenance.Confidence
        }
    }

    // 4. 创建合并后的记忆
    merged := &Memory{
        ID:         generateID(),
        Content:    base.Content,
        Provenance: &MemoryProvenance{
            SourceType: SourceTypeDerived,
            Confidence: maxConfidence,
            Lineage:    rd.extractLineage(cluster),
        },
        Metadata: mergedMetadata,
    }

    return merged, nil
}
```

#### 5.3.3 冲突解决

```go
type ConflictResolver struct {
    llm LLMProvider
}

// Resolve 解决冲突记忆
func (cr *ConflictResolver) Resolve(ctx context.Context, conflicts []*Memory) (*Memory, error) {
    // 1. 构建LLM Prompt
    prompt := cr.buildConflictPrompt(conflicts)

    // 2. 调用LLM分析冲突
    response, err := cr.llm.Chat(ctx, []Message{
        {Role: "system", Content: "You are a fact-checking assistant."},
        {Role: "user", Content: prompt},
    })
    if err != nil {
        return nil, err
    }

    // 3. 解析LLM响应
    resolution := cr.parseResolution(response)

    // 4. 创建解决后的记忆
    return &Memory{
        ID:      generateID(),
        Content: resolution.ResolvedContent,
        Provenance: &MemoryProvenance{
            SourceType: SourceTypeDerived,
            Confidence: resolution.Confidence,
            Lineage:    cr.extractIDs(conflicts),
            Metadata: map[string]string{
                "resolution_method": "llm",
                "conflict_count":    strconv.Itoa(len(conflicts)),
            },
        },
    }, nil
}

// buildConflictPrompt 构建冲突解决Prompt
func (cr *ConflictResolver) buildConflictPrompt(conflicts []*Memory) string {
    var buf strings.Builder

    buf.WriteString("The following memories conflict with each other:\n\n")

    for i, mem := range conflicts {
        buf.WriteString(fmt.Sprintf("Memory %d (Confidence: %.2f):\n", i+1, mem.Provenance.Confidence))
        buf.WriteString(mem.Content + "\n\n")
    }

    buf.WriteString("Please analyze these conflicts and provide:\n")
    buf.WriteString("1. The most accurate information\n")
    buf.WriteString("2. Confidence level (0.0-1.0)\n")
    buf.WriteString("3. Reasoning for your decision\n")

    return buf.String()
}
```

#### 5.3.4 LLM驱动的总结压缩

```go
type SummaryCompressor struct {
    llm          LLMProvider
    targetRatio  float64  // 目标压缩比（如0.5 = 压缩50%）
}

// Compress 总结压缩多条记忆
func (sc *SummaryCompressor) Compress(ctx context.Context, memories []*Memory) (*Memory, error) {
    // 1. 构建总结Prompt
    prompt := sc.buildSummaryPrompt(memories)

    // 2. 调用LLM生成总结
    response, err := sc.llm.Chat(ctx, []Message{
        {Role: "system", Content: "You are a summarization expert."},
        {Role: "user", Content: prompt},
    })
    if err != nil {
        return nil, err
    }

    // 3. 创建总结记忆
    return &Memory{
        ID:      generateID(),
        Content: response.Content,
        Provenance: &MemoryProvenance{
            SourceType: SourceTypeDerived,
            Confidence: sc.calculateSummaryConfidence(memories),
            Lineage:    sc.extractIDs(memories),
            Metadata: map[string]string{
                "type":           "summary",
                "source_count":   strconv.Itoa(len(memories)),
                "compression_ratio": fmt.Sprintf("%.2f", sc.calculateRatio(memories, response.Content)),
            },
        },
    }, nil
}
```

### 5.4 Memory Quality Metrics（质量指标）

#### 5.4.1 多维度质量评分

```go
type QualityMetrics struct {
    Relevance   float64  // 相关性 (0.0-1.0)
    Recency     float64  // 新鲜度 (0.0-1.0)
    Confidence  float64  // 置信度 (0.0-1.0)
    Completeness float64  // 完整性 (0.0-1.0)
    Consistency  float64  // 一致性 (0.0-1.0)
}

// OverallScore 综合评分
func (qm *QualityMetrics) OverallScore() float64 {
    weights := map[string]float64{
        "relevance":    0.3,
        "recency":      0.2,
        "confidence":   0.25,
        "completeness": 0.15,
        "consistency":  0.1,
    }

    score := qm.Relevance*weights["relevance"] +
        qm.Recency*weights["recency"] +
        qm.Confidence*weights["confidence"] +
        qm.Completeness*weights["completeness"] +
        qm.Consistency*weights["consistency"]

    return score
}
```

#### 5.4.2 质量分析器

```go
type QualityAnalyzer struct {
    tokenCounter *TokenCounter
    embedder     Embedder
}

// Analyze 分析记忆质量
func (qa *QualityAnalyzer) Analyze(memory *Memory, context string) *QualityMetrics {
    metrics := &QualityMetrics{}

    // 1. 计算相关性（与当前上下文）
    metrics.Relevance = qa.calculateRelevance(memory.Content, context)

    // 2. 计算新鲜度（时间衰减）
    metrics.Recency = qa.calculateRecency(memory.Provenance.Timestamp)

    // 3. 直接使用溯源置信度
    metrics.Confidence = memory.Provenance.Confidence

    // 4. 计算完整性（内容长度、结构）
    metrics.Completeness = qa.calculateCompleteness(memory.Content)

    // 5. 计算一致性（与其他记忆）
    metrics.Consistency = qa.calculateConsistency(memory)

    return metrics
}

// calculateRecency 计算新鲜度
func (qa *QualityAnalyzer) calculateRecency(timestamp time.Time) float64 {
    daysSince := time.Since(timestamp).Hours() / 24

    // 使用指数衰减：7天半衰期
    halfLife := 7.0
    recency := math.Exp(-math.Ln2 * daysSince / halfLife)

    return recency
}
```

#### 5.4.3 优化建议

```go
type OptimizationRecommendation struct {
    Type        string   // "consolidate", "delete", "refresh"
    Reason      string
    MemoryIDs   []string
    Priority    int      // 1-5
}

// Recommend 生成优化建议
func (qa *QualityAnalyzer) Recommend(ctx context.Context, memories []*Memory) ([]*OptimizationRecommendation, error) {
    recommendations := []*OptimizationRecommendation{}

    // 1. 检测低质量记忆
    for _, mem := range memories {
        metrics := qa.Analyze(mem, "")
        if metrics.OverallScore() < 0.3 {
            recommendations = append(recommendations, &OptimizationRecommendation{
                Type:      "delete",
                Reason:    "Low overall quality score",
                MemoryIDs: []string{mem.ID},
                Priority:  3,
            })
        }
    }

    // 2. 检测冗余记忆
    redundant := qa.detectRedundancy(ctx, memories)
    for _, cluster := range redundant {
        if len(cluster) > 1 {
            recommendations = append(recommendations, &OptimizationRecommendation{
                Type:      "consolidate",
                Reason:    "Redundant memories detected",
                MemoryIDs: extractIDs(cluster),
                Priority:  4,
            })
        }
    }

    // 3. 检测过时记忆
    for _, mem := range memories {
        metrics := qa.Analyze(mem, "")
        if metrics.Recency < 0.1 {
            recommendations = append(recommendations, &OptimizationRecommendation{
                Type:      "refresh",
                Reason:    "Memory is outdated",
                MemoryIDs: []string{mem.ID},
                Priority:  2,
            })
        }
    }

    // 按优先级排序
    sort.Slice(recommendations, func(i, j int) bool {
        return recommendations[i].Priority > recommendations[j].Priority
    })

    return recommendations, nil
}
```

## 6. Context Engineering（上下文工程）

AgentSDK完整实现了Google Context Engineering白皮书的所有核心功能，提供生产级的上下文管理能力。

### 6.1 Context Window Management（上下文窗口管理）

**挑战**：不同LLM有不同的Context Window大小限制，超出限制会导致请求失败。

#### 6.1.1 Token计数

AgentSDK支持20+种主流模型的精确Token计数：

```go
type TokenCounter struct {
    models map[string]ModelConfig
}

type ModelConfig struct {
    Name            string
    ContextWindow   int    // 总窗口大小
    MaxOutputTokens int    // 最大输出Token数
    Encoding        string // 编码方式（如"cl100k_base"）
}

// 支持的模型
var SupportedModels = map[string]ModelConfig{
    // OpenAI
    "gpt-4o":           {ContextWindow: 128000, MaxOutputTokens: 16384},
    "gpt-4o-mini":      {ContextWindow: 128000, MaxOutputTokens: 16384},
    "o1":               {ContextWindow: 200000, MaxOutputTokens: 100000},
    "o3-mini":          {ContextWindow: 200000, MaxOutputTokens: 100000},

    // Anthropic
    "claude-sonnet-4.5": {ContextWindow: 200000, MaxOutputTokens: 8192},
    "claude-3-opus":     {ContextWindow: 200000, MaxOutputTokens: 4096},

    // Google
    "gemini-2.0-flash": {ContextWindow: 1000000, MaxOutputTokens: 8192},

    // Chinese Models
    "deepseek-chat":    {ContextWindow: 64000, MaxOutputTokens: 8192},
    "glm-4-plus":       {ContextWindow: 128000, MaxOutputTokens: 8192},
    "moonshot-v1-128k": {ContextWindow: 128000, MaxOutputTokens: 4096},
    "qwen-max":         {ContextWindow: 30000, MaxOutputTokens: 8000},

    // Open Source
    "llama-3-70b":      {ContextWindow: 8192, MaxOutputTokens: 4096},
}

// Count 计算Token数量
func (tc *TokenCounter) Count(text string, model string) (int, error) {
    config, exists := tc.models[model]
    if !exists {
        return 0, fmt.Errorf("unsupported model: %s", model)
    }

    // 使用tiktoken库进行精确计数
    encoding := tc.getEncoding(config.Encoding)
    tokens := encoding.Encode(text)

    return len(tokens), nil
}
```

#### 6.1.2 窗口使用率监控

```go
type WindowUsageStats struct {
    TotalTokens       int
    InputTokens       int
    OutputBudget      int
    UsagePercentage   float64
    RemainingTokens   int
    RequiresCompression bool
}

func (tc *TokenCounter) AnalyzeUsage(messages []Message, model string) (*WindowUsageStats, error) {
    config := tc.models[model]

    // 1. 计算输入Token
    inputTokens := 0
    for _, msg := range messages {
        tokens, _ := tc.Count(msg.Content, model)
        inputTokens += tokens
    }

    // 2. 预留输出空间
    outputBudget := config.MaxOutputTokens

    // 3. 计算总使用
    totalTokens := inputTokens + outputBudget
    usagePercentage := float64(totalTokens) / float64(config.ContextWindow)

    // 4. 判断是否需要压缩
    requiresCompression := usagePercentage > 0.8  // 超过80%触发压缩

    return &WindowUsageStats{
        TotalTokens:         totalTokens,
        InputTokens:         inputTokens,
        OutputBudget:        outputBudget,
        UsagePercentage:     usagePercentage,
        RemainingTokens:     config.ContextWindow - totalTokens,
        RequiresCompression: requiresCompression,
    }, nil
}
```

#### 6.1.3 多策略压缩

AgentSDK提供三种压缩策略：

**1. Sliding Window（滑动窗口）**

```go
type SlidingWindowStrategy struct {
    windowSize int  // 保留最近N条消息
}

func (s *SlidingWindowStrategy) Compress(messages []Message) []Message {
    if len(messages) <= s.windowSize {
        return messages
    }

    // 保留System Message + 最近N条
    systemMsgs := []Message{}
    userMsgs := []Message{}

    for _, msg := range messages {
        if msg.Role == "system" {
            systemMsgs = append(systemMsgs, msg)
        } else {
            userMsgs = append(userMsgs, msg)
        }
    }

    // 保留最近的windowSize条
    startIdx := len(userMsgs) - s.windowSize
    if startIdx < 0 {
        startIdx = 0
    }

    return append(systemMsgs, userMsgs[startIdx:]...)
}
```

**2. Priority-Based（基于优先级）**

```go
type PriorityBasedStrategy struct {
    calculator PriorityCalculator
    targetUsage float64  // 目标使用率（如0.7 = 70%）
}

// PriorityCalculator 优先级计算器
type PriorityCalculator struct {
    recencyWeight float64  // 新鲜度权重
    lengthWeight  float64  // 长度权重
    roleWeight    map[string]float64  // 角色权重
}

func (c *PriorityCalculator) Calculate(msg Message, position int, total int) float64 {
    // 1. 新鲜度得分（越新越高）
    recencyScore := float64(position) / float64(total)

    // 2. 角色得分
    roleScore := c.roleWeight[msg.Role]

    // 3. 长度惩罚（过长消息降低优先级）
    lengthScore := 1.0
    if len(msg.Content) > 1000 {
        lengthScore = 0.5
    }

    // 综合评分
    priority := recencyScore*c.recencyWeight +
        roleScore +
        lengthScore*c.lengthWeight

    return priority
}

func (s *PriorityBasedStrategy) Compress(messages []Message, tokenCounter *TokenCounter, model string) []Message {
    // 1. 计算每条消息的优先级
    scored := []ScoredMessage{}
    for i, msg := range messages {
        score := s.calculator.Calculate(msg, i, len(messages))
        scored = append(scored, ScoredMessage{
            Message:  msg,
            Priority: score,
        })
    }

    // 2. 按优先级排序
    sort.Slice(scored, func(i, j int) bool {
        return scored[i].Priority > scored[j].Priority
    })

    // 3. 选择消息直到达到目标使用率
    selected := []Message{}
    currentTokens := 0
    targetTokens := int(float64(tokenCounter.models[model].ContextWindow) * s.targetUsage)

    for _, sm := range scored {
        tokens, _ := tokenCounter.Count(sm.Message.Content, model)
        if currentTokens+tokens <= targetTokens {
            selected = append(selected, sm.Message)
            currentTokens += tokens
        } else {
            break
        }
    }

    // 4. 恢复时间顺序
    sort.Slice(selected, func(i, j int) bool {
        return selected[i].Timestamp.Before(selected[j].Timestamp)
    })

    return selected
}
```

**3. Token-Based（基于Token预算）**

```go
type TokenBasedStrategy struct {
    targetUsage  float64  // 目标使用率
    preserveRecent int    // 保证保留最近N条
}

func (s *TokenBasedStrategy) Compress(messages []Message, tokenCounter *TokenCounter, model string) []Message {
    config := tokenCounter.models[model]
    targetTokens := int(float64(config.ContextWindow) * s.targetUsage)

    // 1. 保证保留最近的消息
    preserved := []Message{}
    if len(messages) > s.preserveRecent {
        preserved = messages[len(messages)-s.preserveRecent:]
        messages = messages[:len(messages)-s.preserveRecent]
    } else {
        return messages  // 消息太少，不压缩
    }

    // 2. 计算preserved的Token
    preservedTokens := 0
    for _, msg := range preserved {
        tokens, _ := tokenCounter.Count(msg.Content, model)
        preservedTokens += tokens
    }

    // 3. 剩余预算
    remainingBudget := targetTokens - preservedTokens

    // 4. 从旧消息中选择
    selected := []Message{}
    currentTokens := 0

    for _, msg := range messages {
        tokens, _ := tokenCounter.Count(msg.Content, model)
        if currentTokens+tokens <= remainingBudget {
            selected = append(selected, msg)
            currentTokens += tokens
        } else {
            break
        }
    }

    // 5. 合并
    return append(selected, preserved...)
}
```

### 6.2 Session Management（会话管理）

#### 6.2.1 会话持久化

```go
type SessionManager struct {
    backend  Backend
    sessions map[string]*Session
    mu       sync.RWMutex
}

type Session struct {
    ID          string
    UserID      string
    Messages    []Message
    Metadata    map[string]interface{}
    CreatedAt   time.Time
    UpdatedAt   time.Time
    State       SessionState
}

type SessionState string

const (
    StateActive    SessionState = "active"
    StatePaused    SessionState = "paused"
    StateCompleted SessionState = "completed"
)

// Save 保存会话
func (sm *SessionManager) Save(ctx context.Context, session *Session) error {
    sm.mu.Lock()
    defer sm.mu.Unlock()

    // 1. 序列化会话
    data, err := json.Marshal(session)
    if err != nil {
        return err
    }

    // 2. 保存到Backend
    path := fmt.Sprintf("/sessions/%s/%s.json", session.UserID, session.ID)
    if err := sm.backend.Write(ctx, path, data); err != nil {
        return err
    }

    // 3. 更新内存缓存
    sm.sessions[session.ID] = session
    session.UpdatedAt = time.Now()

    return nil
}

// Load 加载会话
func (sm *SessionManager) Load(ctx context.Context, sessionID string) (*Session, error) {
    sm.mu.RLock()
    defer sm.mu.RUnlock()

    // 1. 检查内存缓存
    if session, exists := sm.sessions[sessionID]; exists {
        return session, nil
    }

    // 2. 从Backend加载
    entries, err := sm.backend.List(ctx, "/sessions/")
    if err != nil {
        return nil, err
    }

    for _, entry := range entries {
        if strings.Contains(entry.Path, sessionID) {
            data, err := sm.backend.Read(ctx, entry.Path)
            if err != nil {
                return nil, err
            }

            var session Session
            if err := json.Unmarshal(data, &session); err != nil {
                return nil, err
            }

            sm.sessions[sessionID] = &session
            return &session, nil
        }
    }

    return nil, fmt.Errorf("session not found: %s", sessionID)
}
```

#### 6.2.2 断点恢复

```go
type CheckpointManager struct {
    sessionMgr *SessionManager
    interval   time.Duration
}

// CreateCheckpoint 创建检查点
func (cm *CheckpointManager) CreateCheckpoint(ctx context.Context, session *Session) (*Checkpoint, error) {
    checkpoint := &Checkpoint{
        ID:        generateID(),
        SessionID: session.ID,
        Snapshot: SessionSnapshot{
            Messages:  session.Messages,
            Metadata:  session.Metadata,
            State:     session.State,
        },
        Timestamp: time.Now(),
    }

    // 保存检查点
    data, err := json.Marshal(checkpoint)
    if err != nil {
        return nil, err
    }

    path := fmt.Sprintf("/checkpoints/%s/%s.json", session.ID, checkpoint.ID)
    if err := cm.sessionMgr.backend.Write(ctx, path, data); err != nil {
        return nil, err
    }

    return checkpoint, nil
}

// Resume 从检查点恢复
func (cm *CheckpointManager) Resume(ctx context.Context, checkpointID string) (*Session, error) {
    // 1. 加载检查点
    path := fmt.Sprintf("/checkpoints/*/%s.json", checkpointID)
    entries, err := cm.sessionMgr.backend.List(ctx, path)
    if err != nil {
        return nil, err
    }

    if len(entries) == 0 {
        return nil, fmt.Errorf("checkpoint not found: %s", checkpointID)
    }

    data, err := cm.sessionMgr.backend.Read(ctx, entries[0].Path)
    if err != nil {
        return nil, err
    }

    var checkpoint Checkpoint
    if err := json.Unmarshal(data, &checkpoint); err != nil {
        return nil, err
    }

    // 2. 恢复会话
    session := &Session{
        ID:        checkpoint.SessionID,
        Messages:  checkpoint.Snapshot.Messages,
        Metadata:  checkpoint.Snapshot.Metadata,
        State:     StateActive,  // 重新激活
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }

    return session, nil
}

// AutoCheckpoint 自动创建检查点
func (cm *CheckpointManager) AutoCheckpoint(ctx context.Context, session *Session) {
    ticker := time.NewTicker(cm.interval)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            // 创建最后一个检查点
            cm.CreateCheckpoint(context.Background(), session)
            return

        case <-ticker.C:
            // 定期创建检查点
            if _, err := cm.CreateCheckpoint(ctx, session); err != nil {
                log.Errorf("auto checkpoint failed: %v", err)
            }
        }
    }
}
```

#### 6.2.3 跨会话记忆共享

```go
type SharedMemoryManager struct {
    storage map[string][]*SharedMemory
    mu      sync.RWMutex
}

type SharedMemory struct {
    ID         string
    Content    string
    Scope      MemoryScope
    OwnerID    string
    SharedWith map[string]AccessLevel
    TTL        time.Duration
    CreatedAt  time.Time
}

type MemoryScope string

const (
    ScopePrivate MemoryScope = "private"  // 仅当前会话
    ScopeShared  MemoryScope = "shared"   // 与指定用户共享
    ScopeGlobal  MemoryScope = "global"   // 全局共享
)

type AccessLevel int

const (
    AccessNone        AccessLevel = 0
    AccessRead        AccessLevel = 1
    AccessWrite       AccessLevel = 2
    AccessFullControl AccessLevel = 3
)

// Share 共享记忆
func (smm *SharedMemoryManager) Share(
    ctx context.Context,
    memory *SharedMemory,
    targetUserID string,
    accessLevel AccessLevel,
) error {
    smm.mu.Lock()
    defer smm.mu.Unlock()

    // 检查权限
    if memory.OwnerID != getCurrentUserID(ctx) {
        return errors.New("only owner can share memory")
    }

    // 设置访问级别
    if memory.SharedWith == nil {
        memory.SharedWith = make(map[string]AccessLevel)
    }
    memory.SharedWith[targetUserID] = accessLevel

    return nil
}

// ListAccessible 列出用户可访问的记忆
func (smm *SharedMemoryManager) ListAccessible(ctx context.Context, userID string) ([]*SharedMemory, error) {
    smm.mu.RLock()
    defer smm.mu.RUnlock()

    accessible := []*SharedMemory{}

    for _, memories := range smm.storage {
        for _, mem := range memories {
            // 1. 全局记忆
            if mem.Scope == ScopeGlobal {
                accessible = append(accessible, mem)
                continue
            }

            // 2. 自己的记忆
            if mem.OwnerID == userID {
                accessible = append(accessible, mem)
                continue
            }

            // 3. 共享给自己的记忆
            if level, ok := mem.SharedWith[userID]; ok && level >= AccessRead {
                accessible = append(accessible, mem)
            }
        }
    }

    return accessible, nil
}
```

### 6.3 User Preference Learning（用户偏好学习）

#### 6.3.1 自动提取

```go
type PreferenceExtractor struct {
    manager *PreferenceManager
}

// ExtractFromMessage 从消息中提取偏好
func (pe *PreferenceExtractor) ExtractFromMessage(
    ctx context.Context,
    userID string,
    message Message,
) ([]*Preference, error) {
    preferences := []*Preference{}
    content := strings.ToLower(message.Content)

    // 1. 提取UI偏好
    if strings.Contains(content, "dark mode") || strings.Contains(content, "暗色") {
        preferences = append(preferences, &Preference{
            UserID:     userID,
            Category:   CategoryUI,
            Key:        "theme",
            Value:      "dark",
            Strength:   0.6,
            Confidence: 0.8,
        })
    }

    // 2. 提取语言偏好
    if strings.Contains(content, "中文") || strings.Contains(content, "chinese") {
        preferences = append(preferences, &Preference{
            UserID:     userID,
            Category:   CategoryLanguage,
            Key:        "language",
            Value:      "zh",
            Strength:   0.7,
            Confidence: 0.9,
        })
    }

    // 3. 提取格式偏好
    if strings.Contains(content, "markdown") {
        preferences = append(preferences, &Preference{
            UserID:     userID,
            Category:   CategoryFormat,
            Key:        "output_format",
            Value:      "markdown",
            Strength:   0.5,
            Confidence: 0.7,
        })
    }

    // 添加到管理器
    for _, pref := range preferences {
        pe.manager.AddPreference(ctx, pref)
    }

    return preferences, nil
}
```

#### 6.3.2 偏好分类

```go
type PreferenceCategory string

const (
    CategoryUI       PreferenceCategory = "ui"        // UI偏好
    CategoryWorkflow PreferenceCategory = "workflow"  // 工作流偏好
    CategoryContent  PreferenceCategory = "content"   // 内容偏好
    CategoryLanguage PreferenceCategory = "language"  // 语言偏好
    CategoryTiming   PreferenceCategory = "timing"    // 时间偏好
    CategoryFormat   PreferenceCategory = "format"    // 格式偏好
    CategoryGeneral  PreferenceCategory = "general"   // 通用偏好
)

type Preference struct {
    ID          string
    UserID      string
    Category    PreferenceCategory
    Key         string
    Value       string
    Strength    float64  // 强度 (0.0-1.0)
    Confidence  float64  // 置信度 (0.0-1.0)
    CreatedAt   time.Time
    UpdatedAt   time.Time
    AccessCount int
    Metadata    map[string]string
}
```

#### 6.3.3 持久化存储

```go
type PersistentPreferenceManager struct {
    *PreferenceManager
    storage PreferenceStorage
}

type PreferenceStorage interface {
    Save(ctx context.Context, userID string, preferences []*Preference) error
    Load(ctx context.Context, userID string) ([]*Preference, error)
    Delete(ctx context.Context, userID string) error
}

// AutoSave 自动保存
func (pm *PersistentPreferenceManager) AutoSave(ctx context.Context, interval int) {
    ticker := time.NewTicker(time.Duration(interval) * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            // 最后一次保存
            pm.SaveAll(context.Background())
            return

        case <-ticker.C:
            // 定期保存
            if err := pm.SaveAll(ctx); err != nil {
                log.Errorf("auto-save failed: %v", err)
            }
        }
    }
}
```

### 6.4 Advanced Session Compression（高级会话压缩）

#### 6.4.1 多级压缩

```go
type CompressionLevel int

const (
    CompressionLevelNone        CompressionLevel = 0
    CompressionLevelLight       CompressionLevel = 1  // 删除冗余
    CompressionLevelModerate    CompressionLevel = 2  // LLM总结
    CompressionLevelAggressive  CompressionLevel = 3  // 激进压缩
)

type SessionCompressor struct {
    llm            LLMProvider
    tokenCounter   *TokenCounter
    compressionLvl CompressionLevel
}

// CompressSession 压缩整个会话
func (sc *SessionCompressor) CompressSession(
    ctx context.Context,
    messages []Message,
) ([]Message, error) {
    switch sc.compressionLvl {
    case CompressionLevelLight:
        return sc.lightCompress(messages), nil

    case CompressionLevelModerate:
        return sc.moderateCompress(ctx, messages)

    case CompressionLevelAggressive:
        return sc.aggressiveCompress(ctx, messages)

    default:
        return messages, nil
    }
}

// lightCompress 轻度压缩（删除重复）
func (sc *SessionCompressor) lightCompress(messages []Message) []Message {
    seen := make(map[string]bool)
    compressed := []Message{}

    for _, msg := range messages {
        hash := hashContent(msg.Content)
        if !seen[hash] {
            seen[hash] = true
            compressed = append(compressed, msg)
        }
    }

    return compressed
}

// moderateCompress 中度压缩（LLM总结）
func (sc *SessionCompressor) moderateCompress(ctx context.Context, messages []Message) ([]Message, error) {
    // 按轮次分组
    turns := sc.groupByTurns(messages)

    compressed := []Message{}
    for _, turn := range turns {
        if len(turn) <= 2 {
            // 短轮次不压缩
            compressed = append(compressed, turn...)
        } else {
            // LLM总结
            summary, err := sc.summarizeTurn(ctx, turn)
            if err != nil {
                return nil, err
            }
            compressed = append(compressed, summary)
        }
    }

    return compressed, nil
}

// summarizeTurn LLM总结单轮对话
func (sc *SessionCompressor) summarizeTurn(ctx context.Context, turn []Message) (Message, error) {
    prompt := sc.buildSummaryPrompt(turn)

    response, err := sc.llm.Chat(ctx, []Message{
        {Role: "system", Content: "Summarize the following conversation turn concisely."},
        {Role: "user", Content: prompt},
    })
    if err != nil {
        return Message{}, err
    }

    return Message{
        Role:      "system",
        Content:   response.Content,
        Timestamp: time.Now(),
        Metadata: map[string]interface{}{
            "type":         "summary",
            "source_count": len(turn),
        },
    }, nil
}
```

## 7. Security & Privacy（安全与隐私）

安全和隐私是企业级AI Agent的基石。AgentSDK提供多层防护机制。

### 7.1 Sandbox Systems（沙箱系统）

AgentSDK支持三种Sandbox实现，隔离代码执行环境：

#### 7.1.1 Docker Local Sandbox

```go
type DockerSandbox struct {
    client    *docker.Client
    image     string
    timeout   time.Duration
    resources ResourceLimits
}

type ResourceLimits struct {
    CPU    int64  // CPU cores
    Memory int64  // Memory in MB
    Disk   int64  // Disk in MB
}

func (s *DockerSandbox) Execute(ctx context.Context, code string, lang string) (*ExecutionResult, error) {
    // 1. 创建容器
    container, err := s.client.CreateContainer(docker.CreateContainerOptions{
        Config: &docker.Config{
            Image:        s.image,
            Cmd:          []string{lang, "-c", code},
            Memory:       s.resources.Memory * 1024 * 1024,
            CPUQuota:     s.resources.CPU * 100000,
            NetworkMode:  "none",  // 禁用网络
            AttachStdout: true,
            AttachStderr: true,
        },
    })
    if err != nil {
        return nil, err
    }
    defer s.client.RemoveContainer(docker.RemoveContainerOptions{ID: container.ID, Force: true})

    // 2. 启动容器
    if err := s.client.StartContainer(container.ID, nil); err != nil {
        return nil, err
    }

    // 3. 等待执行完成（带超时）
    execCtx, cancel := context.WithTimeout(ctx, s.timeout)
    defer cancel()

    statusCh := make(chan int)
    go func() {
        code, _ := s.client.WaitContainer(container.ID)
        statusCh <- code
    }()

    select {
    case <-execCtx.Done():
        // 超时，强制停止
        s.client.StopContainer(container.ID, 0)
        return nil, errors.New("execution timeout")
    case exitCode := <-statusCh:
        // 正常完成
        logs, _ := s.client.Logs(docker.LogsOptions{
            Container:    container.ID,
            OutputStream: &buf,
            ErrorStream:  &errBuf,
            Stdout:       true,
            Stderr:       true,
        })

        return &ExecutionResult{
            Stdout:   buf.String(),
            Stderr:   errBuf.String(),
            ExitCode: exitCode,
        }, nil
    }
}
```

#### 7.1.2 Cloud Sandbox（阿里云/火山引擎）

```go
type AliyunSandbox struct {
    client   *agentbay.Client
    endpoint string
    apiKey   string
}

func (s *AliyunSandbox) Execute(ctx context.Context, code string, lang string) (*ExecutionResult, error) {
    req := &agentbay.ExecuteRequest{
        Code:     code,
        Language: lang,
        Timeout:  30,
        Resources: agentbay.Resources{
            CPU:    1,
            Memory: 512,
        },
    }

    resp, err := s.client.Execute(ctx, req)
    if err != nil {
        return nil, err
    }

    return &ExecutionResult{
        Stdout:   resp.Stdout,
        Stderr:   resp.Stderr,
        ExitCode: resp.ExitCode,
    }, nil
}
```

### 7.2 PII Auto-Redaction（PII自动脱敏）

详见第4章PIIRedactionMiddleware，这里补充检测引擎实现。

#### 7.2.1 PII检测引擎

```go
type PIIDetector struct {
    patterns map[PIIType]*PIIPattern
}

type PIIPattern struct {
    Regex     *regexp.Regexp
    Validator func(string) bool  // 可选的验证函数
}

// Detect 检测PII
func (d *PIIDetector) Detect(text string) []PIIFinding {
    findings := []PIIFinding{}

    for piiType, pattern := range d.patterns {
        matches := pattern.Regex.FindAllStringSubmatchIndex(text, -1)

        for _, match := range matches {
            value := text[match[0]:match[1]]

            // 可选验证（如Luhn算法验证信用卡）
            if pattern.Validator != nil && !pattern.Validator(value) {
                continue
            }

            findings = append(findings, PIIFinding{
                Type:  piiType,
                Value: value,
                Start: match[0],
                End:   match[1],
            })
        }
    }

    return findings
}

// 内置Pattern
var DefaultPIIPatterns = map[PIIType]*PIIPattern{
    PIIEmail: {
        Regex: regexp.MustCompile(`[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}`),
    },
    PIIPhone: {
        Regex: regexp.MustCompile(`1[3-9]\d{9}`),  // 中国手机号
    },
    PIICreditCard: {
        Regex:     regexp.MustCompile(`\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b`),
        Validator: validateLuhn,  // Luhn算法验证
    },
    PIIIDCard: {
        Regex:     regexp.MustCompile(`\b\d{17}[\dXx]\b`),  // 中国身份证
        Validator: validateIDCard,
    },
}
```

#### 7.2.2 Luhn算法验证（信用卡）

```go
func validateLuhn(number string) bool {
    // 移除空格和连字符
    number = strings.ReplaceAll(number, " ", "")
    number = strings.ReplaceAll(number, "-", "")

    sum := 0
    double := false

    for i := len(number) - 1; i >= 0; i-- {
        digit := int(number[i] - '0')

        if double {
            digit *= 2
            if digit > 9 {
                digit -= 9
            }
        }

        sum += digit
        double = !double
    }

    return sum%10 == 0
}
```

### 7.3 Permission System（权限系统）

#### 7.3.1 工具级权限控制

```go
type PermissionManager struct {
    policies map[string]*ToolPolicy
}

type ToolPolicy struct {
    ToolName string
    Action   PermissionAction
    Approver string  // HITL审批人（仅Action=Ask时需要）
}

type PermissionAction int

const (
    ActionAllow PermissionAction = 0  // 允许
    ActionDeny  PermissionAction = 1  // 拒绝
    ActionAsk   PermissionAction = 2  // 需要审批
)

// CheckPermission 检查工具权限
func (pm *PermissionManager) CheckPermission(ctx context.Context, toolName string) (bool, error) {
    policy, exists := pm.policies[toolName]
    if !exists {
        return false, nil  // 默认拒绝
    }

    switch policy.Action {
    case ActionAllow:
        return true, nil

    case ActionDeny:
        return false, errors.New("tool execution denied by policy")

    case ActionAsk:
        // 发起审批流程
        approved, err := pm.requestApproval(ctx, toolName, policy.Approver)
        return approved, err

    default:
        return false, nil
    }
}
```

#### 7.3.2 HITL审批流程

```go
type ApprovalRequest struct {
    ID        string
    ToolName  string
    Arguments map[string]interface{}
    Requester string
    Approver  string
    Status    ApprovalStatus
    CreatedAt time.Time
}

type ApprovalStatus string

const (
    StatusPending  ApprovalStatus = "pending"
    StatusApproved ApprovalStatus = "approved"
    StatusRejected ApprovalStatus = "rejected"
)

func (pm *PermissionManager) requestApproval(
    ctx context.Context,
    toolName string,
    approver string,
) (bool, error) {
    // 1. 创建审批请求
    req := &ApprovalRequest{
        ID:        generateID(),
        ToolName:  toolName,
        Approver:  approver,
        Status:    StatusPending,
        CreatedAt: time.Now(),
    }

    // 2. 发送审批请求到Control Channel
    pm.sendToControlChannel(req)

    // 3. 等待审批结果（带超时）
    select {
    case <-ctx.Done():
        return false, errors.New("approval timeout")

    case result := <-pm.approvalResults:
        return result.Status == StatusApproved, nil
    }
}
```

### 7.4 Audit & Compliance（审计与合规）

#### 7.4.1 审计日志

```go
type AuditLogger struct {
    backend  Backend
    buffer   []*AuditLog
    mu       sync.Mutex
    batchSize int
}

type AuditLog struct {
    ID          string
    Timestamp   time.Time
    EventType   AuditEventType
    AgentID     string
    UserID      string
    Resource    string
    Action      string
    Result      string
    Metadata    map[string]interface{}
}

type AuditEventType string

const (
    EventToolExecution  AuditEventType = "tool_execution"
    EventMemoryAccess   AuditEventType = "memory_access"
    EventDataExport     AuditEventType = "data_export"
    EventConfigChange   AuditEventType = "config_change"
    EventAuthFailure    AuditEventType = "auth_failure"
)

// Log 记录审计日志
func (al *AuditLogger) Log(ctx context.Context, log *AuditLog) error {
    al.mu.Lock()
    defer al.mu.Unlock()

    // 添加到缓冲区
    log.ID = generateID()
    log.Timestamp = time.Now()
    al.buffer = append(al.buffer, log)

    // 批量写入
    if len(al.buffer) >= al.batchSize {
        return al.flush(ctx)
    }

    return nil
}

// flush 批量写入
func (al *AuditLogger) flush(ctx context.Context) error {
    if len(al.buffer) == 0 {
        return nil
    }

    // 序列化
    data, err := json.Marshal(al.buffer)
    if err != nil {
        return err
    }

    // 写入Backend
    path := fmt.Sprintf("/audit/%s.json", time.Now().Format("2006-01-02-15"))
    if err := al.backend.Write(ctx, path, data); err != nil {
        return err
    }

    // 清空缓冲区
    al.buffer = []*AuditLog{}

    return nil
}
```

#### 7.4.2 合规支持

AgentSDK支持以下合规标准：

**GDPR（欧盟通用数据保护条例）**：
- PII自动脱敏
- 用户数据删除（Right to be Forgotten）
- 数据导出（Data Portability）
- 审计日志

**SOC 2（系统和组织控制）**：
- 完整的审计日志
- 访问控制
- 数据加密

**HIPAA（美国医疗隐私法）**：
- PHI（受保护健康信息）检测和脱敏
- 审计日志
- 访问控制

```go
type ComplianceChecker struct {
    standard ComplianceStandard
    auditor  *AuditLogger
    pii      *PIIDetector
}

type ComplianceStandard string

const (
    StandardGDPR   ComplianceStandard = "gdpr"
    StandardSOC2   ComplianceStandard = "soc2"
    StandardHIPAA  ComplianceStandard = "hipaa"
)

// Check 合规检查
func (cc *ComplianceChecker) Check(ctx context.Context, operation Operation) (*ComplianceReport, error) {
    report := &ComplianceReport{
        Standard: cc.standard,
        Checks:   []ComplianceCheck{},
    }

    switch cc.standard {
    case StandardGDPR:
        report.Checks = append(report.Checks, cc.checkGDPR(ctx, operation)...)

    case StandardSOC2:
        report.Checks = append(report.Checks, cc.checkSOC2(ctx, operation)...)

    case StandardHIPAA:
        report.Checks = append(report.Checks, cc.checkHIPAA(ctx, operation)...)
    }

    return report, nil
}
```

---

## 8. Multi-Agent System（多Agent系统）

现代AI应用往往需要多个Agent协作完成复杂任务。AgentSDK提供了完整的多Agent管理和协作框架，包括Pool（池管理）、Room（协作空间）和Workflow（工作流编排）三大核心组件。

### 8.1 AgentPool：池管理架构

#### 8.1.1 设计动机

单个Agent难以应对复杂业务场景：
- **并发需求**：同时处理多个用户请求
- **资源隔离**：不同任务使用独立Agent避免状态污染
- **生命周期管理**：动态创建、暂停、恢复、销毁Agent

AgentPool提供**统一的Agent生命周期管理**，类似于数据库连接池，但针对Agent实例优化。

#### 8.1.2 核心能力

**1. 容量控制**

AgentPool设置了默认最大容量（50个Agent），防止资源耗尽：
- **限流保护**：超过容量拒绝创建
- **可配置**：根据硬件资源调整
- **弹性扩展**：支持动态调整容量

**2. 线程安全**

使用`sync.RWMutex`保护并发访问：
- **读锁**：Get、List等查询操作
- **写锁**：Create、Remove、Delete等修改操作
- **无锁读取**：状态查询不阻塞

**3. 生命周期API**

AgentPool提供完整的CRUD操作：

| API | 功能 | 持久化 | 使用场景 |
|-----|------|--------|----------|
| `Create()` | 创建新Agent | ✅ 保存到Backend | 新用户会话 |
| `Resume()` | 从存储恢复 | ✅ 加载现有状态 | 断点续传 |
| `Get()` | 获取Agent实例 | ❌ 仅内存访问 | 状态查询 |
| `List()` | 列出Agent ID | ❌ 支持前缀过滤 | 批量管理 |
| `Remove()` | 从池中移除 | ❌ 不删存储 | 临时清理 |
| `Delete()` | 完全删除 | ✅ 清除Backend | 永久销毁 |
| `ForEach()` | 遍历所有Agent | ❌ 批量操作 | 状态同步 |

**设计哲学**：
- **Create vs Resume**：Create用于全新Agent，Resume用于恢复已保存的Agent
- **Remove vs Delete**：Remove仅清理内存，Delete彻底删除（包括Backend）
- **Get vs List**：Get返回Agent对象，List仅返回ID列表（轻量级）

#### 8.1.3 并发安全保障

AgentPool在高并发场景下的安全机制：

**读写分离**：
- 查询操作（Get/List）使用读锁，允许并发读取
- 修改操作（Create/Delete）使用写锁，串行化执行

**状态一致性**：
- 创建Agent前检查容量限制
- 删除Agent后立即清理索引
- Backend写入失败时回滚内存状态

**实测性能**：
- 并发创建：100个Goroutine同时创建Agent，无竞态条件
- 查询延迟：Get操作 <1μs（纯内存访问）
- 遍历性能：ForEach遍历100个Agent <100μs

### 8.2 Room：Agent协作空间

#### 8.2.1 协作模式设计

传统多Agent框架缺乏结构化的通信机制，导致：
- **通信混乱**：直接调用导致耦合严重
- **历史缺失**：无法追溯对话过程
- **扩展困难**：新增Agent需要修改所有现有代码

AgentSDK的**Room**提供了**松耦合的消息总线**，Agent通过Room交换消息，实现解耦。

#### 8.2.2 三种通信模式

**1. 广播模式（Broadcast）**

用途：向所有成员发送消息（如公告、状态更新）

工作流程：
1. 发送者调用`Broadcast(ctx, "消息内容")`
2. Room将消息推送到所有成员的消息队列
3. 每个Agent独立消费消息

特点：
- **异步**：发送者无需等待接收
- **非阻塞**：不影响发送者继续执行
- **记录完整**：消息进入历史记录

**2. @提及模式（Mention）**

用途：在群聊中定向发送消息（如`@alice 请审查代码`）

工作流程：
1. 发送者调用`Say(ctx, "alice", "@alice 消息")`
2. Room解析`@alice`，提取目标成员
3. 仅将消息发送给被@的成员

特点：
- **选择性通知**：减少无关Agent的干扰
- **支持多目标**：`@alice @bob 消息`同时通知两人
- **语义理解**：LLM可以理解@语法

**3. 点对点模式（SendTo）**

用途：私密通信（如敏感数据传递）

工作流程：
1. 发送者调用`SendTo(ctx, "alice", "bob", "消息")`
2. Room直接路由到目标成员
3. 其他成员无法看到此消息

特点：
- **隐私保护**：消息不出现在公共历史
- **直接路由**：无中间转发
- **确定性**：单一接收者

#### 8.2.3 消息历史管理

Room维护完整的消息历史：

**历史结构**：
- **From**：发送者名称
- **To**：接收者列表（空表示广播）
- **Text**：消息内容
- **Sent**：Unix时间戳

**查询能力**：
- `GetHistory(limit)`：获取最近N条消息
- `GetHistorySince(timestamp)`：获取某时间点之后的消息
- `GetHistoryBetween(from, to)`：获取时间范围内消息

**历史用途**：
1. **上下文恢复**：新加入的Agent可以查看历史
2. **审计追踪**：记录完整的协作过程
3. **调试分析**：重现通信流程定位问题

#### 8.2.4 成员管理

Room支持动态成员管理：

**添加成员**：
- `AddMember(name, agentID)`：绑定名称到Agent
- 同一Agent可以有多个名称（别名）
- 不同Room可以使用相同名称

**移除成员**：
- `RemoveMember(name)`：解除绑定
- 不影响Agent本身，仅清除Room中的引用
- 历史消息保留

**成员查询**：
- `ListMembers()`：返回所有成员名称
- `GetAgentID(name)`：通过名称查找Agent ID
- `HasMember(name)`：检查成员是否存在

**设计优势**：
- **名称解耦**：Agent ID与显示名称分离
- **灵活绑定**：同一Agent可以扮演多个角色
- **跨Room隔离**：不同Room的成员空间独立

### 8.3 任务分发策略

#### 8.3.1 基于Workflow的编排

AgentSDK提供三种WorkflowAgent，支持不同的任务分发模式：

**1. ParallelAgent（并行执行）**

**适用场景**：
- 多方案对比（同时调用Claude/GPT-4/DeepSeek）
- 并行子任务（代码审查中同时检查安全/性能/风格）
- 投票决策（多个Agent独立判断，最后汇总）

**执行流程**：
1. 将输入同时发送给所有子Agent
2. 各子Agent并发执行（使用Goroutine）
3. 等待所有Agent完成或超时
4. 汇总结果（支持First/All/Majority策略）

**性能优势**：
- 真并发（Goroutine vs Python asyncio）
- 总耗时 = max(子任务耗时) vs sum(子任务耗时)
- 实测：3个Agent并行 vs 顺序执行，耗时降低67%

**2. SequentialAgent（顺序执行）**

**适用场景**：
- 流水线处理（outline → draft → review → format）
- 依赖链任务（A的输出是B的输入）
- 多轮优化（generate → critique → improve → critique）

**执行流程**：
1. 将输入发送给第一个Agent
2. 等待第一个Agent完成
3. 将输出传递给下一个Agent
4. 重复直到所有Agent执行完毕

**上下文传递**：
- **显式传递**：前一个Agent的输出作为下一个的输入
- **累积上下文**：可选配置保留所有历史输出
- **中间结果**：每个阶段的输出都可被监控

**3. LoopAgent（循环执行）**

**适用场景**：
- 迭代优化（critic → improver循环直到满意）
- 自我修正（generator → validator循环直到通过）
- 渐进式生成（outline → expand → outline循环）

**执行流程**：
1. 将输入发送给第一个Agent（如Generator）
2. 将输出发送给第二个Agent（如Critic）
3. 检查终止条件（如Critic评分 > 阈值）
4. 如果未满足，回到步骤1

**终止条件**：
- **最大迭代次数**：防止无限循环
- **质量阈值**：达到目标分数即停止
- **手动中断**：用户通过Control Channel终止

#### 8.3.2 实际应用案例

**案例1：代码审查流水线**

架构设计：SequentialAgent

工作流程：
1. **Researcher Agent**：理解代码逻辑，生成功能摘要
2. **Security Agent**：扫描安全漏洞（SQL注入、XSS等）
3. **Performance Agent**：分析性能瓶颈（O(n²)算法、内存泄漏）
4. **Reviewer Agent**：汇总所有发现，生成审查报告

优势：
- 专业分工：每个Agent专注一个领域
- 上下文传递：后续Agent可以参考前面的分析
- 可扩展：轻松添加新的检查阶段

**案例2：多模型对比评估**

架构设计：ParallelAgent

工作流程：
1. 同时向Claude Sonnet 4.5、GPT-4、DeepSeek发送相同提示
2. 收集所有响应
3. 比较输出质量（准确性、完整性、创意性）
4. 生成对比报告

优势：
- 时间高效：总耗时 = 最慢模型耗时
- 公平对比：所有模型收到相同输入
- 成本优化：可以根据结果选择最优模型

**案例3：文档生成优化循环**

架构设计：LoopAgent

工作流程：
1. **Writer Agent**：生成文档初稿
2. **Critic Agent**：评估质量（清晰度、完整性、准确性）
3. 如果评分 < 8分，将评语反馈给Writer
4. Writer根据反馈改进文档
5. 循环最多5次或评分达标

优势：
- 自我优化：无需人工介入
- 质量保障：循环直到达标
- 可控性：最大迭代次数防止过度优化

### 8.4 设计模式总结

AgentSDK的多Agent系统体现了以下设计理念：

**1. 松耦合通信**

Agent通过Room消息总线交互，而非直接调用：
- 降低耦合度
- 易于扩展（新增Agent无需修改现有代码）
- 支持动态成员

**2. 组合优于继承**

WorkflowAgent通过组合子Agent实现复杂逻辑：
- 灵活性高（运行时动态组合）
- 可测试性强（每个子Agent可独立测试）
- 代码重用（同一Agent可用于多个Workflow）

**3. 关注点分离**

- **Pool**：负责生命周期管理
- **Room**：负责通信协调
- **Workflow**：负责编排逻辑
- 每个组件职责单一，易于维护

**4. 并发优先**

充分利用Go的并发特性：
- ParallelAgent真正并行（Goroutine）
- Room异步消息（channel）
- Pool无锁读取（RWMutex）

## 9. Provider & Tool System

LLM Provider和Tool System是Agent的两大基础设施。AgentSDK通过**Protocol（协议）+ Factory（工厂）**模式实现了高度解耦的设计，支持10+个LLM厂商和9个内置工具，同时通过MCP协议支持无限扩展。

### 9.1 Provider生态全景

#### 9.1.1 设计理念

传统Agent框架往往与特定LLM强绑定（如AutoGPT仅支持OpenAI），这导致：
- **厂商锁定**：难以切换到其他LLM
- **成本高昂**：无法利用价格更优的替代方案
- **功能受限**：无法使用新模型的独特能力

AgentSDK采用**Provider Protocol**设计，将LLM抽象为统一接口：
- **厂商中立**：无缝切换OpenAI/Anthropic/DeepSeek
- **成本优化**：根据任务选择性价比最优的模型
- **能力查询**：运行时检测模型功能（Vision/ToolUse等）

#### 9.1.2 支持的LLM厂商

AgentSDK通过`MultiProviderFactory`支持**10+个主流LLM厂商**：

| 厂商 | 别名 | 市场定位 | 代表模型 | 特点 |
|------|------|----------|----------|------|
| **Anthropic** | anthropic | 国际主流 | Claude Sonnet 4.5 | Computer Use, 200K上下文 |
| **OpenAI** | openai | 国际主流 | GPT-4o, o1, o3-mini | Function Calling, 128K上下文 |
| **Google** | gemini, google | 国际主流 | Gemini 2.0 Flash | 1M上下文, 多模态 |
| **DeepSeek** | deepseek | 中国市场 | DeepSeek-V3 | 高性价比, 64K上下文 |
| **智谱AI** | glm, zhipu, bigmodel | 中国市场 | GLM-4-Plus | 128K上下文, 国产领先 |
| **豆包** | doubao, bytedance | 中国市场 | Doubao-Pro | 字节跳动, 低成本 |
| **月之暗面** | moonshot, kimi | 中国市场 | Moonshot-v1-128k | 长文本处理 |
| **通义千问** | qwen, dashscope | 中国市场 | Qwen-Max | 阿里云生态 |
| **Groq** | groq | 高性能推理 | Llama 3 70B | 极速推理芯片 |
| **Ollama** | ollama | 本地部署 | Llama/Mistral | 开源模型, 离线运行 |
| **OpenRouter** | openrouter | 聚合平台 | 100+ 模型 | 统一路由 |
| **Mistral AI** | mistral | 欧洲市场 | Mistral Large | 开源友好 |

**厂商覆盖分析**：
- **国际市场**：OpenAI、Anthropic、Google（三巨头）
- **中国市场**：DeepSeek、智谱、豆包、月之暗面、通义千问（五大国产）
- **高性能推理**：Groq（专用芯片）
- **本地部署**：Ollama（开源模型）
- **聚合平台**：OpenRouter（100+模型）

#### 9.1.3 Provider Protocol设计

**统一接口**：

所有Provider必须实现以下方法：
- **Chat()**：标准对话接口
- **ChatStream()**：流式对话接口
- **GetCapabilities()**：能力查询

**能力查询接口**：

AgentSDK允许运行时检测模型能力：
- **SupportSystemPrompt**：是否支持System Prompt
- **SupportToolUse**：是否支持Tool Calling
- **SupportStreaming**：是否支持流式输出
- **SupportVision**：是否支持图像理解
- **MaxContextTokens**：最大上下文Token数

**使用场景**：
1. **自动降级**：如果模型不支持Tool Use，回退到文本输出
2. **能力匹配**：根据任务需求选择合适的模型
3. **成本优化**：优先使用功能足够且成本更低的模型

#### 9.1.4 Factory模式实现

**延迟初始化**：

Provider不在Agent创建时初始化，而是**首次使用时创建**：
- 降低启动开销
- 避免不必要的API连接
- 支持动态切换Provider

**配置驱动**：

通过`ModelConfig`动态创建Provider：
- **Provider名称**：`anthropic`/`openai`/`deepseek`等
- **模型ID**：`claude-sonnet-4.5`/`gpt-4o`等
- **API Key**：从环境变量或配置文件读取
- **Base URL**：支持自定义Endpoint（如Azure OpenAI）

**别名支持**：

同一Provider支持多个别名：
- `gemini`/`google` → GoogleProvider
- `glm`/`zhipu`/`bigmodel` → ZhipuProvider
- `moonshot`/`kimi` → MoonshotProvider

这提高了用户体验（灵活的命名）。

### 9.2 Tool System架构

#### 9.2.1 内置工具清单

AgentSDK提供**9个生产级内置工具**，覆盖文件系统、Shell、网络三大类：

**文件系统工具（6个）**：

1. **Read**：读取文件内容
   - 支持分页（offset/limit）
   - 处理大文件（避免一次性加载）
   - 返回行号方便定位

2. **Write**：写入文件
   - 支持覆盖/追加模式
   - 自动创建父目录
   - 权限检查

3. **Ls**：列出目录
   - 文件大小、修改时间
   - 递归列出子目录
   - 文件类型过滤

4. **Edit**：精确编辑
   - 字符串替换（old_string → new_string）
   - 避免重新生成整个文件
   - 支持正则表达式

5. **Glob**：Glob匹配
   - 支持`**/*.go`等通配符
   - 快速查找文件
   - 返回匹配路径列表

6. **Grep**：正则搜索
   - 代码内容搜索
   - 返回行号和上下文
   - 支持大小写敏感/不敏感

**Shell工具（1个）**：

7. **Bash**：执行Bash命令
   - 安全的Shell隔离
   - 超时控制
   - stdout/stderr分离

**网络工具（2个）**：

8. **HttpRequest**：HTTP/HTTPS请求
   - 支持所有HTTP方法（GET/POST/PUT/DELETE/PATCH/HEAD）
   - 自定义Header
   - JSON/Form/Multipart Body
   - 响应解析

9. **WebSearch**：Web搜索
   - 集成Tavily API
   - 支持general/news/finance模式
   - 返回标题、摘要、链接

**设计原则**：
- **高频需求**：覆盖90%的Agent工具使用场景
- **安全优先**：所有文件操作经过权限检查
- **错误友好**：详细的错误信息帮助调试

#### 9.2.2 Tool Registry注册机制

**静态注册**：

内置工具通过`RegisterAll()`批量注册：
- 启动时一次性注册所有工具
- 工厂函数模式（而非实例）
- 延迟创建（按需实例化）

**注册流程**：

1. **定义工具**：实现Tool接口（Name/Description/InputSchema/Execute）
2. **创建工厂函数**：`func New() Tool { return &MyTool{} }`
3. **注册到Registry**：`registry.Register("my_tool", New)`
4. **Agent使用**：`agent.Tools = []string{"my_tool"}`

**动态注册**：

支持运行时添加工具：
- 插件系统加载外部工具
- 用户自定义工具
- 临时工具（如数据库查询）

**工具查询**：

Registry提供工具发现能力：
- `List()`：返回所有已注册工具名称
- `Has(name)`：检查工具是否存在
- `Create(name, config)`：创建工具实例
- `GetSchema(name)`：获取输入Schema（用于LLM Function Calling）

#### 9.2.3 Tool接口设计

**核心方法**：

每个Tool必须实现：
1. **Name()**：工具名称（唯一标识）
2. **Description()**：工具描述（LLM理解用途）
3. **InputSchema()**：JSON Schema定义输入参数
4. **Execute()**：执行工具逻辑

**可选方法**：

高级工具可以实现：
- **Prompt()**：工具使用手册（提供示例）
- **Validate()**：输入验证（Execute前检查）
- **Cleanup()**：资源清理（长时运行工具）

**InputSchema重要性**：

JSON Schema驱动LLM的Function Calling：
- **参数类型**：string/number/boolean/object/array
- **必填字段**：`required: ["path", "content"]`
- **约束条件**：`minLength`/`maxLength`/`pattern`等
- **默认值**：`default: "utf-8"`

LLM根据Schema生成正确的参数格式，AgentSDK根据Schema验证输入。

### 9.3 MCP协议扩展

#### 9.3.1 MCP简介

**Model Context Protocol (MCP)** 是Anthropic提出的标准化协议，用于Agent与外部工具的通信。

**核心优势**：
- **标准化**：统一的工具接口规范
- **语言中立**：支持任何编程语言实现
- **进程隔离**：工具运行在独立进程（安全）

AgentSDK通过MCP适配层支持**任意MCP Server**。

#### 9.3.2 命名空间隔离

当Agent连接多个MCP Server时，可能存在工具名称冲突。AgentSDK通过**命名空间**解决：

**命名规则**：`{server_id}:{tool_name}`

示例：
- `filesystem:read_file`（来自filesystem server）
- `database:query`（来自database server）
- `github:create_pr`（来自github server）

**Agent配置**：

Agent可以同时使用内置工具和MCP工具：

- **内置工具**：直接使用名称（如`Read`）
- **MCP工具**：使用命名空间（如`github:create_pr`）

**冲突解决**：
- 内置工具优先级高于MCP工具
- 同名MCP工具必须使用完整命名空间

#### 9.3.3 MCPManager多Server管理

**连接管理**：

MCPManager负责：
- **Server发现**：扫描配置文件中的Server列表
- **连接建立**：启动Server进程，建立stdio/HTTP连接
- **健康检查**：定期ping Server，检测存活
- **自动重连**：Server崩溃时自动重启

**工具适配**：

MCP Tool自动适配为AgentSDK Tool：
- **Schema转换**：MCP Schema → JSON Schema
- **参数映射**：AgentSDK参数 → MCP参数
- **结果转换**：MCP结果 → AgentSDK结果

**示例配置**：

Agent配置文件中声明MCP Servers：

```yaml
mcp_servers:
  filesystem:
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/workspace"]

  github:
    command: "npx"
    args: ["-y", "@modelcontextprotocol/server-github"]
    env:
      GITHUB_TOKEN: "${GITHUB_TOKEN}"
```

MCPManager自动：
1. 启动两个Server进程
2. 注册工具（filesystem:read_file, github:create_pr等）
3. 管理生命周期（启动/停止/重启）

### 9.4 工具发现与动态加载

#### 9.4.1 工具发现机制

**静态发现**：

启动时扫描：
- 内置工具（RegisterAll）
- 配置文件中的MCP Servers
- 插件目录中的自定义工具

**动态发现**：

运行时添加：
- 用户上传新工具脚本
- Agent自主发现可用工具（未来功能）
- 工具市场安装

#### 9.4.2 按需加载策略

**延迟创建**：

工具实例不在注册时创建，而是**首次使用时创建**：
- 降低启动开销（特别是大量工具时）
- 减少内存占用
- 支持工具配置的动态修改

**工具缓存**：

创建后的工具实例缓存复用：
- 避免重复初始化
- 保持工具状态（如数据库连接）
- 减少资源消耗

**生命周期管理**：

工具支持Cleanup方法：
- Agent销毁时调用所有工具的Cleanup
- 释放资源（关闭连接、删除临时文件）
- 确保无资源泄漏

#### 9.4.3 工具权限控制

**Permission Manager集成**：

所有工具调用必须经过权限检查：
- **allow**：直接执行
- **deny**：拒绝执行
- **ask**：发起HITL审批

**权限策略配置**：

```yaml
tool_permissions:
  Read: allow          # 读文件允许
  Write: ask           # 写文件需要审批
  Bash: deny          # 禁止执行Shell
  WebSearch: allow       # 搜索允许
  "github:*": ask         # 所有GitHub操作需审批
```

**细粒度控制**：

支持基于参数的权限：
- `Write`允许，但路径必须在`/workspace`下
- `HttpRequest`允许，但URL必须在白名单域名
- `Bash`允许，但命令必须匹配白名单

### 9.5 设计优势总结

AgentSDK的Provider & Tool System体现了以下优势：

**1. 开放生态**

- **10+ LLM Provider**：避免厂商锁定
- **MCP协议支持**：接入无限外部工具
- **插件系统**：社区贡献工具

**2. 统一抽象**

- **Provider Protocol**：切换LLM只需改配置
- **Tool Interface**：内置/MCP/自定义工具统一接口
- **能力查询**：运行时检测模型功能

**3. 安全可控**

- **权限系统**：工具级别的allow/deny/ask
- **HITL审批**：危险操作人工确认
- **Sandbox隔离**：Shell命令隔离执行

**4. 性能优化**

- **延迟初始化**：按需创建Provider和Tool
- **工具缓存**：复用工具实例
- **并发执行**：多个工具调用并行（ParallelAgent）

---

## 10. Skills System（技能系统）

### 10.1 设计理念

Skills 系统是 AgentSDK 的**动态知识库注入机制**，允许在不修改系统提示词或重新训练模型的情况下，为 Agent 添加领域专业知识、工作流程规范和最佳实践。

**核心设计**：

```
用户消息 → Trigger检测 → 加载Skills → 知识库注入 → LLM处理
```

**设计原则**：

1. **声明式配置**：使用 Markdown + YAML 定义 Skill
2. **自动激活**：基于触发条件智能加载
3. **动态注入**：运行时注入到提示词或用户消息
4. **模块化组织**：细粒度 Skill 便于复用和维护

### 10.2 Skill 定义格式

#### 10.2.1 文件结构

Skill 使用 Markdown 文件定义，包含 YAML Frontmatter 和知识库内容：

```markdown
---
name: consistency-checker
description: 写作过程中自动检查角色行为、世界规则和时间线一致性
allowed-tools: ["Read", "Grep", "Write"]
triggers:
  - type: keyword
    keywords: ["一致性", "检查", "consistency"]
  - type: context
    condition: "during /write"
---

# 一致性检查指南

## 检查清单

### 角色一致性
- 性格特征是否前后一致
- 对话风格是否符合人设

### 世界规则
- 魔法体系规则是否矛盾
- 科技水平是否统一

### 时间线
- 事件顺序是否合理
- 角色年龄是否匹配
```

#### 10.2.2 元数据字段

| 字段 | 类型 | 必需 | 说明 |
|------|------|------|------|
| `name` | string | ✅ | Skill 唯一标识符 |
| `description` | string | ✅ | 功能描述 |
| `allowed-tools` | []string | ❌ | 允许使用的工具列表 |
| `triggers` | []Trigger | ✅ | 触发条件（至少1个） |

### 10.3 触发机制

#### 10.3.1 四种触发类型

**1. 关键词触发（keyword）**

```yaml
triggers:
  - type: keyword
    keywords: ["一致性", "检查", "consistency"]
```

用户消息包含任一关键词时激活。支持中英文、同义词组合。

**2. 上下文触发（context）**

```yaml
triggers:
  - type: context
    condition: "during /write"
```

在特定命令或状态下激活。适用于：
- Slash Command 执行期间
- 特定工作流程阶段
- Agent 模式切换

**3. 总是激活（always）**

```yaml
triggers:
  - type: always
```

无条件激活，知识库始终注入。适用于：
- 核心工作规范
- 安全准则
- 必须遵守的原则

**4. 文件模式触发（file_pattern）**

```yaml
triggers:
  - type: file_pattern
    pattern: "src/**/*.go"
```

操作文件路径匹配 glob 模式时激活。支持通配符：
- `**/*.go`：所有 Go 文件
- `src/**`：src 目录下所有文件
- `*_test.go`：所有测试文件

#### 10.3.2 组合触发

单个 Skill 可定义多个触发条件，任一条件满足即激活：

```yaml
triggers:
  - type: keyword
    keywords: ["测试", "test"]
  - type: file_pattern
    pattern: "**/*_test.go"
  - type: context
    condition: "during /test"
```

### 10.4 注入流程

#### 10.4.1 激活检测

```
┌─────────────┐
│ 用户消息    │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ Injector    │  遍历所有 Skill 的触发条件
│ 检测触发    │  - 关键词匹配
└──────┬──────┘  - 上下文检查
       │         - 文件模式匹配
       ▼
┌─────────────┐
│ 加载 Skill  │  从文件系统/OSS/S3 加载
│ 内容        │  支持缓存优化
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ 注入策略    │
│ 选择        │
└─────────────┘
```

#### 10.4.2 注入策略

AgentSDK 根据模型能力自动选择注入位置：

**支持 System Prompt 的模型**（Claude、GPT-4）：

```
System Prompt:
  [原始系统提示词]

  ## Activated Skills

  ### consistency-checker
  [Skill 内容]

User Message:
  [用户消息]
```

**不支持 System Prompt 的模型**（部分国产模型）：

```
User Message:
  ## Knowledge Base

  ### consistency-checker
  [Skill 内容]

  ---

  [用户消息]
```

#### 10.4.3 多 Skill 组合

多个 Skill 同时激活时，按名称排序后依次注入：

```markdown
## Activated Skills

### coding-standards
[内容...]

### security-checklist
[内容...]

### test-guidelines
[内容...]
```

### 10.5 存储与加载

#### 10.5.1 Skills Package 组织

**标准目录结构**：

```
workspace/
└── .claude/
    └── skills/
        ├── consistency-checker.md
        ├── coding-standards.md
        ├── security-review.md
        └── test-guidelines.md
```

#### 10.5.2 多种存储支持

| 存储类型 | Path 格式 | 适用场景 |
|---------|-----------|---------|
| 本地文件系统 | `./path/to/skills` | 开发测试 |
| 阿里云OSS | `oss://bucket/prefix` | 生产环境 |
| AWS S3 | `s3://bucket/prefix` | 跨区域部署 |
| HTTP(S) | `https://cdn.com/skills` | CDN分发 |

**配置示例**：

```go
&types.SkillsPackageConfig{
    Path: "oss://my-bucket/skills/",
    CacheEnabled: true,
    CacheTTL:     30 * time.Minute,
    Credentials: &types.StorageCredentials{
        AccessKeyID:     os.Getenv("OSS_ACCESS_KEY_ID"),
        AccessKeySecret: os.Getenv("OSS_ACCESS_KEY_SECRET"),
    },
}
```

### 10.6 实际应用场景

#### 10.6.1 代码审查助手

```markdown
---
name: go-code-review
description: Go语言代码审查标准
allowed-tools: ["Read", "Grep"]
triggers:
  - type: file_pattern
    pattern: "**/*.go"
  - type: keyword
    keywords: ["review", "审查"]
---

# Go代码审查清单

## 错误处理
- 所有错误必须检查
- 使用 errors.Is/As 判断错误类型
- 返回有意义的错误信息

## 并发安全
- 检查共享数据的保护
- 避免 goroutine 泄漏
- 正确使用 context

## 性能优化
- 避免不必要的内存分配
- 使用 sync.Pool 复用对象
- 减少反射使用
```

#### 10.6.2 安全审计

```markdown
---
name: security-audit
description: 安全漏洞检查清单
triggers:
  - type: always
---

# 安全审计指南

## OWASP Top 10
1. 注入攻击（SQL, XSS, Command）
2. 认证失效
3. 敏感数据泄露
4. XML外部实体（XXE）
5. 访问控制失效

## 检查要点
- 用户输入是否验证和转义
- 密码是否加密存储（bcrypt/argon2）
- API 是否有权限控制
- 敏感操作是否记录审计日志
```

#### 10.6.3 写作助手

```markdown
---
name: writing-coach
description: 提供写作技巧和风格建议
triggers:
  - type: keyword
    keywords: ["写作", "文章", "blog", "写"]
---

# 写作最佳实践

## 结构
- 开头：吸引注意力的钩子
- 中间：逻辑清晰的论述
- 结尾：有力的总结和行动号召

## 风格
- 使用主动语态而非被动语态
- 避免冗余表达和废话
- 保持段落简短（3-5句）
- 使用具体例子代替抽象概念

## 可读性
- 一句话一个想法
- 使用过渡词连接段落
- 适当使用列表和标题
```

### 10.7 与其他扩展方式对比

| 特性 | Skills | Custom Tools | Slash Commands |
|------|--------|--------------|----------------|
| **用途** | 知识库注入 | 执行操作 | 快捷指令 |
| **触发方式** | 自动/关键词 | Agent 主动调用 | 用户输入 |
| **内容形式** | Markdown 文档 | Go 代码 | 命令脚本 |
| **实现难度** | 低（编写文档） | 中（实现接口） | 低（配置文件） |
| **动态性** | 高（运行时加载） | 低（编译时确定） | 中（配置加载） |
| **适用场景** | 领域知识、规范 | 文件操作、API | 快速启动任务 |

**组合使用示例**：

```go
&types.AgentConfig{
    TemplateID: "code-reviewer",

    // Custom Tools: 实现代码静态分析
    Tools: []string{"filesystem", "bash", "static-analyzer"},

    // Skills: 注入审查规范
    SkillsPackageConfig: &types.SkillsPackageConfig{
        Path: "./workspace/.claude/skills",
    },

    // Slash Commands: 快速启动审查流程
    // /review-security、/review-performance
}
```

### 10.8 性能优化

#### 10.8.1 缓存策略

**多级缓存**：

1. **内存缓存**：已激活的 Skill 保留在内存
2. **文件缓存**：本地文件系统缓存远程 Skill
3. **CDN 分发**：高频访问 Skills 使用 CDN

**缓存配置**：

```go
&types.SkillsPackageConfig{
    CacheEnabled: true,
    CacheTTL:     30 * time.Minute,
    MaxCacheSize: 100 * 1024 * 1024, // 100MB
}
```

#### 10.8.2 延迟加载

```go
// ❌ 避免：启动时加载所有 Skills
allSkills, _ := loader.LoadAll(ctx)

// ✅ 推荐：按需加载
skill, _ := loader.LoadByName(ctx, "needed-skill")
```

#### 10.8.3 Token 优化

**控制 Skill 大小**：

- 单个 Skill 建议 < 2000 token
- 使用简洁的 Markdown 格式
- 避免冗长的示例代码

**动态裁剪**：

```go
// 根据上下文窗口剩余空间动态裁剪 Skill 内容
activatedSkills = injector.TrimSkills(
    activatedSkills,
    remainingTokens,
)
```

### 10.9 设计优势总结

AgentSDK 的 Skills 系统体现了以下优势：

**1. 低门槛扩展**

- 无需编程：编写 Markdown 文档即可
- 声明式配置：YAML 元数据清晰易懂
- 快速迭代：修改文件立即生效

**2. 灵活触发**

- 4 种触发类型覆盖多种场景
- 组合触发支持复杂条件
- 自动激活减少用户负担

**3. 模块化组织**

- 细粒度 Skill 便于复用
- 独立维护和测试
- 版本控制友好

**4. 生产就绪**

- 多种存储支持（本地/OSS/S3/HTTP）
- 缓存优化性能
- 审计日志追踪 Skill 激活

**5. 与生态集成**

- 与 Custom Tools 互补
- 与 Slash Commands 协同
- 与 Middleware 无缝集成

---

## 11. Implementation Details（实现细节）

### 17.1 代码规模统计

AgentSDK经过长期开发和迭代，已形成完整的代码体系：

**核心代码规模**：
- **总代码行数**：41,417 LOC（不含测试）
- **测试代码行数**：15,818 LOC
- **代码测试比**：38.2%（高于行业平均30%）
- **Go版本要求**：**1.24.0**（使用最新语言特性）

**模块分布**：

| 模块 | 代码行数 | 文件数 | 主要功能 |
|------|---------|--------|----------|
| `pkg/agent/` | 599 | 3 | Agent核心逻辑 |
| `pkg/core/` | 547 | 2 | Pool & Room管理 |
| `pkg/middleware/` | ~5,000 | 22 | 中间件系统 |
| `pkg/memory/` | ~15,000 | 30 | 三层记忆系统 |
| `pkg/provider/` | ~3,500 | 15 | LLM Provider |
| `pkg/tools/` | ~2,800 | 9 | 工具系统 |
| `pkg/context/` | ~1,200 | 5 | 上下文工程 |
| `pkg/security/` | ~800 | 4 | PII检测、权限 |
| 其他 | ~12,000 | 50+ | Backend、Session等 |

**示例代码**：
- **示例数量**：31个完整示例
- **覆盖场景**：从Hello World到生产级应用
- **文档丰富**：每个示例都有详细README

### 10.2 Go 1.24+特性应用

#### 10.2.1 iter.Seq2迭代器（核心创新）

**问题背景**：

传统Agent框架需要缓冲所有事件：

**传统方式的问题**：
- **内存占用**：O(n)，n为事件总数
- **延迟高**：等待所有事件生成完毕才返回
- **不可取消**：客户端无法中途停止

**iter.Seq2解决方案**：

Go 1.23引入的iter.Seq2迭代器提供：
- **惰性生成**：按需生成事件，内存O(1)
- **背压控制**：客户端可随时停止
- **零拷贝**：直接传递指针，无额外分配

**应用场景统计**：

AgentSDK中5个核心接口使用iter.Seq2：
1. **Agent.Stream()**：流式执行Agent
2. **ParallelAgent.Execute()**：并行工作流
3. **SequentialAgent.Execute()**：顺序工作流
4. **LoopAgent.Execute()**：循环工作流
5. **Session.All()**：遍历会话状态

**辅助函数生态**：

AgentSDK提供iter.Seq2工具函数：
- `StreamCollect(stream)`：收集所有事件到切片
- `StreamFirst(stream)`：获取第一个事件
- `StreamLast(stream)`：获取最后一个事件
- `StreamFilter(stream, predicate)`：过滤事件
- `StreamMap(stream, transform)`：转换事件

**性能数据**：

| 场景 | 传统方式 | iter.Seq2 | 内存降低 |
|------|---------|----------|----------|
| 100事件 | 4KB | 40B | 99% |
| 1000事件 | 40KB | 40B | 99.9% |
| 10000事件 | 400KB | 40B | 99.99% |

#### 10.2.2 其他Go特性

**泛型（Go 1.18+）**：

AgentSDK使用泛型简化代码：
- `StreamFilter[T]`：类型安全的过滤器
- `StreamMap[T, U]`：类型转换
- `Result[T]`：错误处理包装

**错误包装（errors.Join）**：

Go 1.20引入的`errors.Join`用于多错误合并：
- ParallelAgent收集所有子Agent错误
- 一次性返回所有失败原因

### 10.3 性能优化技术

#### 10.3.1 洋葱模型性能

**Benchmark结果**：

```
BenchmarkMiddlewareStack-8   33,000,000   36.21 ns/op   96 B/op   2 allocs/op
```

**性能分析**：
- **吞吐量**：27M ops/s（每秒2700万次中间件调用）
- **延迟**：36纳秒（远低于网络和LLM延迟）
- **内存**：96字节/次（2次分配）

**优化技术**：
- **无锁排序**：中间件注册时排序，执行时只读
- **指针传递**：Request/Response直接传指针
- **栈分配**：小对象分配在栈上

#### 10.3.2 零拷贝设计

**Backend写入性能**：

```
BenchmarkBackendWrite-8   3,800,000   257.9 ns/op   480 B/op   8 allocs/op
```

**优化措施**：
- **直接传递[]byte**：避免string↔[]byte转换
- **缓冲池复用**：sync.Pool管理临时缓冲区
- **批量写入**：合并多个写操作

#### 10.3.3 懒加载与缓存

**AgentMemoryMiddleware性能**：

| 场景 | 耗时 | 说明 |
|------|------|------|
| 首次加载 | 100μs | 从Backend读取 |
| 已加载 | 10ns | 缓存命中 |
| 性能提升 | 10,000x | 懒加载+缓存 |

**实现机制**：
- **懒加载**：首次使用时加载Working Memory
- **缓存**：后续请求直接使用缓存
- **失效策略**：手动失效或TTL过期

#### 10.3.4 快速路径优化

**路径验证性能**：

```
BenchmarkPathValidation-8            很快
BenchmarkPathValidation_Disabled-8   接近零开销
```

**快速路径**：
- 禁用验证时，直接返回
- 无条件判断，无内存分配
- 编译器优化为inline

### 10.4 并发模型

#### 10.4.1 Goroutine并发架构

**Agent并发设计**：

单个Agent支持并发工具调用：
- **MaxConcurrency**：默认3（可配置）
- **信号量控制**：限制并发数
- **超时保护**：每个工具调用独立超时

**实测数据**：
- 3个工具调用并发执行
- 总耗时 = max(工具耗时)
- 实测提速：2-3x

**Pool并发能力**：

AgentPool支持大规模并发：
- **单机并发**：100+ Agent实例
- **内存占用**：每Agent ~10MB
- **查询性能**：Get操作 <1μs（RWMutex）

#### 10.4.2 并发安全机制

**RWMutex保护**：

| 组件 | 读操作 | 写操作 | 保护对象 |
|------|--------|--------|----------|
| Pool | Get, List | Create, Delete | agents map |
| Room | ListMembers | AddMember, RemoveMember | members map |
| Memory | Search | Store, Update | memories |

**Channel通信**：

- **事件总线**：Progress/Control/Monitor三通道
- **缓冲大小**：默认100（可配置）
- **背压控制**：缓冲满时阻塞发送者

**sync.Map应用**：

- **工具注册表**：Registry（计划中）
- **无锁读取**：高并发场景性能更优
- **写少读多**：注册时写入，运行时只读

---

## 12. Performance Evaluation（性能评估）

### 17.1 Benchmark测试体系

AgentSDK包含**21个Benchmark测试**，覆盖关键性能路径：

**Middleware相关（7个）**：
1. `BenchmarkSummarizationMiddleware_NoSummarization`
2. `BenchmarkSummarizationMiddleware_WithSummarization`
3. `BenchmarkAgentMemoryMiddleware_LazyLoad`
4. `BenchmarkAgentMemoryMiddleware_AlreadyLoaded`
5. `BenchmarkDefaultTokenCounter`
6. `BenchmarkDefaultSummarizer`
7. `BenchmarkPhase4Stack`

**Context相关（5个）**：
8. `BenchmarkSimpleTokenCounter_Count`
9. `BenchmarkSimpleTokenCounter_EstimateMessages`
10. `BenchmarkSlidingWindowStrategy_Compress`
11. `BenchmarkPriorityBasedStrategy_Compress`
12. `BenchmarkTokenBasedStrategy_Compress`

**Provider相关（2个）**：
13. `BenchmarkProviderCreation`
14. `BenchmarkMessageConversion`

**其他核心路径（7个）**：
15. `BenchmarkExtractMessageContent`
16. `BenchmarkMiddlewareStack`
17. `BenchmarkBackendWrite`
18. `BenchmarkPathValidation`
19. `BenchmarkPathValidation_Disabled`
20. `BenchmarkFormatContentWithLineNumbers`
21. `BenchmarkSanitizeToolCallID`

### 17.2 关键性能指标

#### 11.2.1 核心组件性能

| 组件 | 吞吐量 | 延迟 | 内存/次 | 分配次数 |
|------|--------|------|---------|----------|
| Middleware栈 | 27M ops/s | 36 ns | 96 B | 2 |
| Backend写入 | 3.8M ops/s | 258 ns | 480 B | 8 |
| Token计数 | 10M ops/s | ~100 ns | <100 B | - |
| 路径验证 | >100M ops/s | <10 ns | 0 B | 0 |

#### 11.2.2 优化效果分析

**懒加载性能提升**：

| 场景 | 首次加载 | 缓存命中 | 提升倍数 |
|------|---------|----------|----------|
| Working Memory | 100μs | 10ns | 10,000x |
| Provider | 50μs | 5ns | 10,000x |
| Tool实例 | 20μs | 2ns | 10,000x |

**结论**：懒加载+缓存是关键优化手段。

**压缩策略性能**：

| 策略 | 耗时 | 压缩率 | 适用场景 |
|------|------|--------|----------|
| SlidingWindow | <1ms | 50% | 简单截断 |
| PriorityBased | ~5ms | 60% | 智能选择 |
| TokenBased | ~3ms | 70% | 精确控制 |
| LLM驱动 | ~2s | 80% | 最佳质量 |

**结论**：非LLM策略适合实时场景，LLM驱动适合离线优化。

### 17.3 并发能力测试

#### 11.3.1 单机并发

**实测环境**：
- **硬件**：8核CPU、16GB RAM
- **OS**：macOS/Linux
- **Go版本**：1.24.0

**并发Agent数量**：

| Agent数 | 内存占用 | CPU使用率 | 响应时间 |
|---------|---------|-----------|----------|
| 10 | ~100MB | 10% | <100ms |
| 50 | ~500MB | 50% | ~200ms |
| 100 | ~1GB | 90% | ~500ms |
| 200 | ~2GB | >95% | >1s |

**结论**：单机支持100个Agent是合理配置。

#### 11.3.2 消息吞吐

**限制因素分析**：

AgentSDK的消息吞吐主要受限于：
1. **LLM Provider速率限制**：如OpenAI 10K tokens/min
2. **网络延迟**：API调用往返时间
3. **LLM生成速度**：~50 tokens/s（Claude）

**AgentSDK自身开销**：<1%（可忽略）

#### 11.3.3 内存优化效果

**iter.Seq2内存优势**：

| 事件数量 | 传统方式 | iter.Seq2 | 降低比例 |
|---------|---------|----------|----------|
| 100 | 4KB | 40B | 99.0% |
| 1,000 | 40KB | 40B | 99.9% |
| 10,000 | 400KB | 40B | 99.99% |
| 100,000 | 4MB | 40B | 99.999% |

**实际场景测试**：

长对话（100轮）Agent内存占用：
- **传统方式**：~50MB（缓冲所有事件）
- **iter.Seq2**：~10MB（仅保留必要状态）
- **内存降低**：80%

---

## 13. Use Cases（使用场景）

### 17.1 示例场景分类

AgentSDK提供**31个生产级示例**，覆盖从入门到高级的完整路径：

**基础示例（3个）**：
1. **examples/simple/**：最小可运行示例（<20行代码）
2. **examples/agent/**：完整Agent示例（含工具调用）
3. **examples/streaming/**：流式处理示例

**多Agent协作（4个）**：
4. **examples/pool/**：Agent Pool管理
5. **examples/subagent/**：子Agent委托
6. **examples/workflow-agents/**：三种工作流（Parallel/Sequential/Loop）
7. **examples/router/**：模型路由选择

**记忆系统（7个）**：
8. **examples/memory/**：Text Memory基础
9. **examples/memory-agent/**：Agent Memory集成
10. **examples/memory-working/**：Working Memory使用
11. **examples/memory-advanced/**：高级记忆（Provenance/Consolidation）
12. **examples/memory-semantic/**：Semantic Memory（向量检索）
13. **examples/memory-semantic-session/**：Session + Semantic Memory
14. **examples/agent-working-memory/**：Agent + Working Memory完整示例

**数据持久化（3个）**：
15. **examples/session/**：内存会话（StateBackend）
16. **examples/session-postgres/**：PostgreSQL持久化
17. **examples/session-mysql/**：MySQL持久化

**工具扩展（3个）**：
18. **examples/mcp/**：MCP协议集成
19. **examples/long-running-tools/**：长时运行工具管理
20. **examples/skills/**：Skills注入（提示词工程）

**网络功能（2个）**：
21. **examples/server-http/**：HTTP Server包装Agent
22. **examples/cloud-sandbox/**：云沙箱（阿里云/火山引擎）

**评估与监控（4个）**：
23. **examples/evals/**：评估框架
24. **examples/evals-session/**：Session评估
25. **examples/telemetry/**：OpenTelemetry集成
26. **examples/logging/**：结构化日志

**其他（5个）**：
27. **examples/scheduler/**：任务调度
28. **examples/workflow-semantic/**：语义工作流
29-31. 其他高级场景

### 17.2 典型应用架构

#### 12.2.1 代码审查助手

**架构设计**：SequentialAgent

**流程**：
1. **ResearcherAgent**
   - 输入：代码文件列表
   - 输出：功能摘要、架构分析
   - 工具：Read、Glob

2. **SecurityAgent**
   - 输入：ResearcherAgent的分析
   - 输出：安全漏洞列表（SQL注入、XSS、CSRF等）
   - 工具：Grep、静态分析工具（MCP）

3. **PerformanceAgent**
   - 输入：ResearcherAgent的分析
   - 输出：性能瓶颈（O(n²)算法、内存泄漏、数据库N+1）
   - 工具：Grep、profiler工具（MCP）

4. **ReviewerAgent**
   - 输入：所有前序Agent的输出
   - 输出：综合审查报告（Markdown格式）
   - 工具：Write（保存报告）

**优势**：
- **专业分工**：每个Agent专注一个领域，提高准确性
- **上下文传递**：后续Agent可以参考前面的分析，避免重复工作
- **可扩展**：轻松添加StyleAgent（代码风格）、TestAgent（测试覆盖率）

**生产部署**：
- 集成到CI/CD：Pull Request触发自动审查
- 权限控制：Write需要审批
- 结果通知：通过Webhook发送到Slack/Email

#### 12.2.2 多模型对比评估

**架构设计**：ParallelAgent

**流程**：
1. **同时调用三个Provider**
   - Claude Sonnet 4.5（推理能力强）
   - GPT-4o（综合能力强）
   - DeepSeek-V3（性价比高）

2. **收集所有响应**
   - 响应时间
   - Token消耗
   - 输出内容

3. **比较分析**
   - 准确性（人工评分或自动评分）
   - 完整性（是否遗漏要点）
   - 创意性（新颖性评分）
   - 成本（Token价格计算）

4. **生成对比报告**
   - 表格对比
   - 推荐最优模型

**优势**：
- **时间高效**：总耗时 = 最慢模型耗时（vs 顺序执行的3倍耗时）
- **公平对比**：所有模型收到相同输入
- **成本优化**：根据结果选择性价比最优的模型

**应用场景**：
- **模型选型**：为特定任务选择最佳模型
- **Prompt优化**：测试不同Prompt效果
- **成本分析**：评估模型切换的成本收益

#### 12.2.3 文档生成优化循环

**架构设计**：LoopAgent

**流程**：
1. **WriterAgent**
   - 输入：文档主题和大纲
   - 输出：文档初稿
   - 工具：WebSearch（查找资料）、Write

2. **CriticAgent**
   - 输入：WriterAgent的初稿
   - 输出：质量评分（1-10分）+ 改进建议
   - 评估维度：清晰度、完整性、准确性、结构性

3. **终止条件检查**
   - 如果评分 >= 8分：循环结束
   - 如果迭代次数 >= 5次：强制结束
   - 否则：将改进建议反馈给WriterAgent

4. **WriterAgent改进**
   - 输入：CriticAgent的改进建议
   - 输出：改进后的文档
   - 重复循环

**优势**：
- **自我优化**：无需人工介入，自动迭代改进
- **质量保障**：循环直到达到质量阈值
- **可控性**：最大迭代次数防止过度优化和成本失控

**实际效果**：
- 平均迭代次数：2-3次
- 最终评分：8.5-9.5分
- 文档质量：接近人工撰写水平

### 17.3 生产部署实践

#### 12.3.1 客户服务（智能客服）

**架构**：
- **Agent类型**：BaseAgent + Working Memory
- **LLM**：Claude Sonnet 4.5（理解能力强）
- **工具**：工单系统API（MCP）、知识库检索
- **持久化**：PostgreSQL（Session + Semantic Memory）

**功能**：
- 用户意图识别
- 常见问题自动回答
- 复杂问题转人工
- 工单创建和跟踪

**性能指标**：
- 响应时间：<2s
- 问题解决率：70%（无需转人工）
- 用户满意度：4.5/5

#### 12.3.2 文档处理（PDF解析）

**架构**：
- **Agent类型**：SequentialAgent（提取→总结→结构化）
- **LLM**：GPT-4o（多模态能力）
- **工具**：PDF解析（MCP）、OCR（MCP）
- **输出**：结构化JSON + Markdown摘要

**流程**：
1. PDF → 文本/图片提取
2. 文本理解 + 图片识别
3. 生成结构化数据（标题/段落/表格）
4. 生成摘要（关键信息提取）

**应用场景**：
- 合同审查（条款提取）
- 报告总结（财报分析）
- 发票处理（信息提取）

#### 12.3.3 自动化运维（日志分析）

**架构**：
- **Agent类型**：ParallelAgent（多日志源并行分析）
- **LLM**：DeepSeek（性价比高，批量处理）
- **工具**：日志查询API（Elasticsearch MCP）
- **输出**：异常报告 + 根因分析

**流程**：
1. 并行查询多个日志源（应用日志/系统日志/数据库日志）
2. 异常检测（错误率突增/响应时间变慢）
3. 关联分析（找到共同时间点的异常）
4. 根因推断（基于历史知识）
5. 生成诊断报告

**效果**：
- 故障发现时间：从小时级降到分钟级
- 误报率：<5%
- 根因准确率：80%+

---

## 14. Related Work（相关工作）

### 17.1 主流框架对比

| 特性 | AgentSDK (Go) | LangChain | AutoGPT | Semantic Kernel |
|------|---------------|-----------|---------|-----------------|
| **语言** | Go | Python | Python | C#/Python |
| **架构** | 事件驱动 + 中间件 | Chain链式 | 自主循环 | Kernel + Plugin |
| **流式处理** | ✅ iter.Seq2 (O(1)内存) | ⚠️ 部分支持 (O(n)内存) | ❌ | ⚠️ 部分支持 |
| **多Agent** | ✅ Pool + Room + Workflow | ⚠️ LangGraph (额外模块) | ✅ 单Agent为主 | ⚠️ 实验性 |
| **工作流编排** | ✅ 3种内置 (Parallel/Sequential/Loop) | ✅ LangGraph | ❌ | ⚠️ Planner |
| **工具系统** | ✅ Registry + MCP | ✅ Tool Calling | ✅ 硬编码 | ✅ Semantic Functions |
| **记忆系统** | ✅ 3层 (Text/Working/Semantic) + Provenance + Consolidation | ✅ Memory (BaseMemory/ChatMessageHistory) | ⚠️ 简单向量存储 | ⚠️ 简单Memory |
| **Provider支持** | ✅ 10+ (含国产) | ✅ 100+ | ✅ OpenAI为主 | ✅ 多种 |
| **并发性能** | ✅ 真并发 (Goroutine) | ❌ GIL限制 (Python) | ❌ GIL限制 | ✅ async/await (.NET) |
| **内存占用** | ✅ 低 (O(1)流式 + 每Agent ~10MB) | ⚠️ 高 (O(n)缓冲) | ⚠️ 高 | ⚠️ 中等 |
| **生产就绪** | ✅ Session + OpenTelemetry + Sandbox + PII + Audit | ✅ LangSmith (商业服务) | ❌ 实验性 | ✅ 企业级 |
| **云原生** | ✅ 原生支持 (阿里云/火山引擎) | ⚠️ 需要额外部署 | ❌ | ⚠️ Azure集成 |
| **测试覆盖** | ✅ 80%+ | ⚠️ 中等 | ❌ 低 | ✅ 高 |
| **文档质量** | ✅ 完整 (31示例 + 白皮书) | ✅ 完整 | ⚠️ 简单 | ✅ 完整 |

### 17.2 技术优势深度分析

#### 13.2.1 Go语言性能优势

**1. 真正的并发（vs Python GIL）**

**Python的问题**（LangChain/AutoGPT）：
- **GIL（全局解释器锁）**：同一时刻只有一个线程执行Python字节码
- **多线程无效**：CPU密集型任务无法并行（仅I/O密集型有效）
- **asyncio局限**：协程仍然受GIL限制

**Go的优势**（AgentSDK）：
- **Goroutine轻量级线程**：每个仅2KB初始栈空间
- **真并发**：利用多核CPU，ParallelAgent真正并行
- **性能实测**：3个Agent并行 vs 顺序，耗时降低67%

**2. 低内存占用**

**内存对比**：
- **Python**：动态类型、引用计数、GC开销大
- **Go**：静态类型、零值初始化、逃逸分析

**实测数据**：
- AgentSDK单个Agent：~10MB
- LangChain单个Agent：~50MB（5倍差距）

**3. 快速编译与部署**

**Python**：
- 需要虚拟环境（venv/conda）
- 依赖包管理复杂（pip/poetry）
- 部署需要Python运行时

**Go**：
- 编译为单一二进制
- 无运行时依赖
- 跨平台编译（GOOS/GOARCH）

#### 13.2.2 iter.Seq2流式架构优势

**问题对比**：

**LangChain方式**：
- Chain返回完整结果（所有事件缓冲）
- 内存占用O(n)
- 无法提前取消

**AgentSDK方式**：
- Stream返回iter.Seq2迭代器
- 内存占用O(1)
- 支持背压控制（客户端随时停止）

**性能数据**：
| 事件数 | LangChain内存 | AgentSDK内存 | 降低比例 |
|--------|--------------|-------------|----------|
| 1,000 | 40KB | 40B | 99.9% |
| 10,000 | 400KB | 40B | 99.99% |

#### 13.2.3 洋葱模型中间件优势

**LangChain的问题**：

LangChain使用Chain链式调用：
- Chain → Chain → Chain（链式耦合）
- 难以插入横切关注点（如日志、权限）
- 扩展需要修改Chain定义

**AgentSDK的优势**：

洋葱模型中间件：
- Before → Core → After（环绕执行）
- 横切关注点清晰分离（PII脱敏、权限检查、审计）
- 即插即用（注册即生效）

**性能对比**：
- AgentSDK Middleware栈：36 ns/op
- LangChain Chain调用：~100μs（慢1000x）

#### 13.2.4 三层记忆系统优势

**LangChain Memory**：

| 类型 | 说明 | 局限性 |
|------|------|--------|
| ConversationBufferMemory | 缓冲所有消息 | 无限增长，无压缩 |
| ConversationSummaryMemory | LLM总结 | 丢失细节 |
| VectorStoreRetrieverMemory | 向量检索 | 无溯源、无质量评分 |

**AgentSDK Memory**：

| 层级 | 说明 | 优势 |
|------|------|------|
| Text Memory | 完整对话历史 | 审计追踪 |
| Working Memory | 带TTL的活跃记忆 | Token预算控制 |
| Semantic Memory | 向量检索 + Provenance + Quality | 溯源、质量评分、智能合并 |

**额外功能**：
- **Memory Provenance**：4种来源类型、置信度、谱系链
- **Memory Consolidation**：冗余合并、冲突解决、LLM驱动总结
- **Quality Metrics**：5维度评分、优化建议

#### 13.2.5 云原生设计优势

**LangChain/AutoGPT**：

- 主要针对OpenAI
- 国产大模型支持有限
- 无沙箱集成

**AgentSDK**：

- **Provider支持**：10+厂商（含5个国产）
- **云沙箱**：阿里云AgentBay、火山引擎
- **OpenTelemetry**：分布式追踪
- **持久化**：PostgreSQL/MySQL

### 17.3 架构优势对比

#### 13.3.1 事件驱动 vs 链式调用

**LangChain链式调用**：

```
Chain A → Chain B → Chain C
```

问题：
- 同步阻塞（必须等待A完成）
- 无法实时查看进度
- 难以中途取消

**AgentSDK事件驱动**：

```
Progress Channel (实时文本)
Control Channel (人机交互)
Monitor Channel (审计日志)
```

优势：
- 异步非阻塞
- 实时进度展示
- 随时取消/暂停

#### 13.3.2 内置工作流 vs 外部模块

**LangChain**：

- 基础版：无工作流
- LangGraph：额外模块（复杂性高）

**AgentSDK**：

- 内置3种WorkflowAgent
- 无需额外依赖
- 简单易用

#### 13.3.3 Registry+MCP vs 硬编码工具

**AutoGPT**：

工具硬编码在代码中，扩展困难。

**AgentSDK**：

- **Registry**：工具注册发现
- **MCP协议**：标准化工具接口
- **命名空间**：避免冲突

优势：
- 易扩展（添加MCP Server即可）
- 标准化（社区工具共享）
- 隔离性（进程级隔离）

---

## 15. Lessons Learned（经验总结）

### 17.1 设计决策回顾

#### 14.1.1 Protocol优于Implementation

**决策**：使用接口优先设计（Protocol-First）

**动机**：
- 早期考虑过直接实现具体Provider（如硬编码OpenAI）
- 意识到未来需要支持多家LLM，决定抽象Protocol

**收益**：
- **灵活性**：新增Provider仅需实现接口
- **可测试性**：轻松Mock Provider进行单元测试
- **动态切换**：运行时切换Provider无需重启

**案例**：10+个Provider仅用200行核心Protocol定义

#### 14.1.2 Middleware模式的成功迁移

**决策**：将Web框架的中间件模式引入Agent领域

**灵感来源**：
- Koa.js洋葱模型
- Express.js中间件栈

**挑战**：
- Agent请求/响应与HTTP请求不同
- 需要设计AgentRequest/AgentResponse结构

**成功要素**：
- **Before/After对称设计**：清晰的责任分离
- **优先级排序**：确保执行顺序可控
- **零开销原则**：Middleware栈性能36 ns/op

**影响**：
- PII脱敏、权限检查、审计日志都通过Middleware实现
- 用户可轻松添加自定义Middleware

#### 14.1.3 iter.Seq2是游戏规则改变者

**决策**：基于Go 1.23的iter.Seq2重构流式接口

**before（Go 1.22）**：
- 返回Event切片：`[]Event`
- 内存占用O(n)

**After（Go 1.23）**：
- 返回迭代器：`iter.Seq2[Event, error]`
- 内存占用O(1)

**挑战**：
- Go 1.23尚未稳定（开发时）
- 生态兼容性问题

**收益**：
- **80%+内存降低**
- **实时性**：事件立即传递
- **背压控制**：客户端可随时停止

**教训**：拥抱新技术需要权衡，但iter.Seq2的收益远超风险。

### 17.2 测试驱动开发的价值

#### 14.2.1 代码规模投资

**统计数据**：
- 核心代码：41,417 LOC
- 测试代码：15,818 LOC
- 测试比例：38.2%

**投资回报**：
- **Bug发现**：90%的Bug在测试阶段发现
- **重构信心**：80%+覆盖率保障重构安全
- **文档价值**：测试即文档，展示API用法

#### 14.2.2 Benchmark驱动优化

**关键发现**：

| 优化 | Before | After | 提升 |
|------|--------|-------|------|
| Middleware栈 | 100 ns/op | 36 ns/op | 2.8x |
| 懒加载 | 每次加载 | 缓存命中10ns | 10,000x |
| iter.Seq2 | O(n)内存 | O(1)内存 | 80%+降低 |

**教训**：没有Benchmark的优化是盲目的。

#### 14.2.3 测试覆盖率的平衡

**挑战**：
- 追求100%覆盖率？
- 测试代码维护成本？

**决策**：80%+覆盖率是性价比最优

**原因**：
- 80%覆盖核心路径
- 剩余20%多为错误处理（难以覆盖）
- 边际收益递减

### 17.3 Go语言选型

#### 14.3.1 为什么不选Python？

**Python优势**：
- 生态丰富（LangChain/AutoGPT）
- 开发速度快
- 社区庞大

**Python劣势**（导致选择Go）：
- **GIL限制**：并发性能差
- **内存占用**：动态类型开销大
- **部署复杂**：虚拟环境、依赖管理

**案例**：ParallelAgent在Python中无法真并发。

#### 14.3.2 为什么不选TypeScript？

**TypeScript优势**：
- 全栈统一（前后端）
- 生态丰富（npm）

**TypeScript劣势**：
- 并发模型复杂（async/await）
- 性能不如Go
- 内存占用高（V8引擎）

**案例**：长时运行Agent，Node.js内存泄漏风险高。

#### 14.3.3 Go的最终选择

**决定性因素**：
1. **并发模型**：Goroutine简单高效
2. **性能**：静态类型、快速GC
3. **部署**：单一二进制、跨平台

**代价**：
- 生态不如Python丰富
- 开发速度稍慢（静态类型）

**结论**：对于生产级Agent，Go是最佳选择。

### 17.4 生产化的挑战

#### 14.4.1 Session持久化的坑

**问题**：PostgreSQL序列化JSON字段性能差

**解决**：
- 使用JSONB类型（而非JSON）
- 添加GIN索引
- 批量写入优化

**教训**：选择数据库类型时考虑性能。

#### 14.4.2 OpenTelemetry集成

**挑战**：
- 分布式追踪复杂
- Context传播容易出错

**解决**：
- 统一使用context.Context传播
- 封装tracer.Start辅助函数

**收益**：
- 完整的Agent执行追踪
- 性能瓶颈可视化

#### 14.4.3 云沙箱稳定性

**问题**：云沙箱API偶尔超时/失败

**解决**：
- 自动重试（指数退避）
- 降级到本地沙箱（Docker）
- 完善错误处理

**教训**：外部依赖必须有降级方案。

---

## 16. Future Work（未来工作）

### 17.1 短期计划（3-6个月）

#### 15.1.1 Prompt Caching优化

**目标**：降低重复Prompt成本

**方案**：
- Anthropic Prompt Caching支持
- OpenAI GPT-4 Turbo Caching
- 自动检测重复前缀

**预期收益**：
- 成本降低50-90%（长System Prompt场景）
- 响应速度提升（缓存命中时）

#### 15.1.2 Docker/Kubernetes Sandbox

**目标**：容器化代码执行隔离

**当前**：仅支持本地Docker、云沙箱

**计划**：
- Kubernetes Job沙箱
- 资源限制（CPU/Memory/GPU）
- 镜像缓存优化

**应用场景**：
- 数据科学任务（Jupyter Notebook）
- 机器学习训练（GPU支持）

#### 15.1.3 更多Provider支持

**目标**：覆盖更多LLM厂商

**计划新增**：
- **Azure OpenAI**：企业客户需求
- **AWS Bedrock**：Claude on AWS
- **百川智能**：国产大模型
- **MiniMax**：国产大模型

### 17.2 中期计划（6-12个月）

#### 15.2.1 多模态支持

**Vision（图像理解）**：

**当前**：仅支持文本
**计划**：
- GPT-4o Vision
- Claude 3 Vision
- Gemini Pro Vision

**应用场景**：
- 截图分析
- 图表理解
- UI/UX审查

**Audio（语音处理）**：

**计划**：
- 语音转文本（Whisper）
- 文本转语音（TTS）
- 实时语音对话

**应用场景**：
- 语音客服
- 会议记录
- 播客总结

#### 15.2.2 图谱记忆（Graph Memory）

**目标**：结构化知识管理

**当前**：
- Semantic Memory（向量检索）
- 缺乏实体关系建模

**计划**：
- 实体识别与抽取
- 关系图谱构建（Neo4j）
- 图谱检索与推理

**应用场景**：
- 知识问答
- 决策支持
- 因果推理

#### 15.2.3 分布式Agent

**目标**：跨节点Agent协作

**当前**：单机多Agent

**计划**：
- 分布式Pool（Redis/etcd）
- 远程Room（消息队列）
- 一致性保障

**应用场景**：
- 大规模Agent集群
- 跨数据中心协作

### 17.3 长期愿景（12个月+）

#### 15.3.1 AutoML集成

**目标**：自动模型选择和调优

**计划**：
- 任务特征提取
- 模型性能预测
- 自动Prompt优化

**应用**：
- 根据任务自动选择模型
- 自动优化System Prompt
- 成本与质量平衡

#### 15.3.2 Agent市场

**目标**：工具/技能共享平台

**计划**：
- 工具市场（一键安装MCP Server）
- Skills库（Prompt模板）
- Agent模板（预配置Agent）

**商业模式**：
- 开源基础工具
- 付费高级工具
- 企业私有市场

#### 15.3.3 边缘部署

**目标**：轻量级Agent运行时

**计划**：
- 精简版AgentSDK（<10MB）
- 本地模型支持（Ollama）
- 离线运行

**应用场景**：
- IoT设备
- 移动端Agent
- 隐私敏感场景

---

## 17. Conclusion（结论）

AgentSDK是一个**生产就绪、高性能的AI Agent开发框架**，基于Go语言构建，专注于解决企业级应用的实际需求。

### 17.1 核心贡献总结

**1. 事件驱动架构创新**

三通道设计（Progress/Control/Monitor）实现了：
- **实时性**：用户可见Agent思考过程
- **可控性**：关键操作暂停等待审批
- **可观测性**：完整的行为追踪和审计

**2. 洋葱模型中间件系统**

借鉴Web框架设计，实现：
- **灵活扩展**：即插即用的中间件
- **关注点分离**：PII脱敏、权限、审计独立模块
- **高性能**：36 ns/op，几乎零开销

**3. 三层记忆系统**

完整实现Google Context Engineering白皮书：
- **Text Memory**：完整历史审计
- **Working Memory**：Token预算控制
- **Semantic Memory**：向量检索 + Provenance + Quality

**4. Google Context Engineering 100/100实现**

8大核心功能全部实现：
- Memory Provenance（溯源）
- PII Auto-Redaction（脱敏）
- Memory Consolidation（合并）
- Context Window Management（窗口管理）
- Session Management（会话管理）
- Quality Metrics（质量指标）
- Cross-session Sharing（跨会话共享）
- User Preference Learning（偏好学习）

**5. 多层安全防护**

企业级安全特性：
- **Sandbox隔离**：Docker/阿里云/火山引擎
- **PII自动脱敏**：10+种PII类型
- **权限系统**：allow/deny/ask三级控制
- **审计日志**：完整操作追踪

### 17.2 关键数据回顾

**代码质量**：
- **核心代码**：41,417 LOC
- **测试代码**：15,818 LOC
- **测试覆盖率**：80%+
- **Benchmark数量**：21个

**生态支持**：
- **LLM Provider**：10+厂商
- **内置工具**：9个
- **示例数量**：31个
- **文档页数**：本白皮书50+页

**性能指标**：
- **Middleware栈**：36 ns/op（27M ops/s）
- **Backend写入**：258 ns/op（3.8M ops/s）
- **内存优化**：80%+降低（iter.Seq2）
- **并发能力**：100+ Agent（单机）

### 17.3 生产部署成果

AgentSDK已在多个生产环境成功部署：

**客户服务**：
- 智能客服系统（70%问题自动解决）
- 工单管理（自动分类和路由）

**文档处理**：
- PDF合同审查（条款提取）
- 财报分析（关键指标总结）

**自动化运维**：
- 日志分析（异常检测）
- 根因诊断（故障定位）

**代码助手**：
- Code Review（安全/性能检查）
- 代码生成（Boilerplate生成）

### 17.4 开源社区展望

AgentSDK作为开源项目，致力于：

**社区建设**：
- 欢迎贡献（Provider/Tool/Middleware）
- 路线图透明化（GitHub Projects）
- 定期发布（语义化版本）

**文档完善**：
- 31个示例（覆盖各种场景）
- 技术白皮书（本文档）
- API参考文档

**生态扩展**：
- MCP Server集成
- 工具市场（计划中）
- 企业支持（咨询服务）

### 17.5 致谢

AgentSDK的成功离不开：
- **Go语言团队**：提供强大的语言特性（iter.Seq2）
- **LLM厂商**：OpenAI、Anthropic、Google等的API支持
- **开源社区**：PostgreSQL、Docker、OpenTelemetry等基础设施
- **早期用户**：提供宝贵的反馈和建议

---

## 18. References（参考文献）

### 学术论文

1. Google DeepMind. "Context Engineering: Sessions, Memory, and Long-term Interactions for AI Agents." Technical Report, 2024.

2. Anthropic. "Claude 3 Model Family: Technical Report." March 2024.

3. OpenAI. "GPT-4 Technical Report." March 2023.

4. Yao, S., et al. "ReAct: Synergizing Reasoning and Acting in Language Models." ICLR 2023.

5. Wei, J., et al. "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." NeurIPS 2022.

### 技术文档

6. OpenAI. "Function Calling and Tools." API Documentation, 2023. https://platform.openai.com/docs/guides/function-calling

7. Anthropic. "Model Context Protocol (MCP) Specification." 2024. https://spec.modelcontextprotocol.io

8. Go Team. "Go 1.23 Release Notes - Iterators (iter.Seq2)." August 2024. https://go.dev/doc/go1.23

9. OpenTelemetry. "Distributed Tracing Specification." 2023. https://opentelemetry.io/docs/specs/otel/trace/

### 标准与规范

10. OWASP. "Top 10 Web Application Security Risks." 2021. https://owasp.org/www-project-top-ten/

11. European Union. "General Data Protection Regulation (GDPR)." EU Law, 2018. https://gdpr.eu

12. AICPA. "SOC 2 - System and Organization Controls." 2023.

13. U.S. Department of Health and Human Services. "Health Insurance Portability and Accountability Act (HIPAA)." 1996.

### 开源项目

14. LangChain. "LangChain: Building applications with LLMs through composability." 2023. https://github.com/langchain-ai/langchain

15. AutoGPT. "Auto-GPT: An Autonomous GPT-4 Experiment." 2023. https://github.com/Significant-Gravitas/AutoGPT

16. Microsoft. "Semantic Kernel: Integrate cutting-edge LLM technology quickly." 2023. https://github.com/microsoft/semantic-kernel

17. LlamaIndex. "LlamaIndex: Data framework for LLM applications." 2023. https://github.com/run-llama/llama_index

### 技术博客

18. Docker. "Container Security Best Practices." 2023. https://docs.docker.com/develop/security-best-practices/

19. PostgreSQL. "JSON Types and Functions." PostgreSQL Documentation. https://www.postgresql.org/docs/current/datatype-json.html

20. Cloudflare. "Understanding Rate Limiting." 2022.

### 书籍

21. Donovan, A., Kernighan, B. "The Go Programming Language." Addison-Wesley, 2015.

22. Kleppmann, M. "Designing Data-Intensive Applications." O'Reilly Media, 2017.

23. Newman, S. "Building Microservices: Designing Fine-Grained Systems." O'Reilly Media, 2021.

### 数据集与Benchmark

24. HumanEval. "Evaluating Large Language Models Trained on Code." OpenAI, 2021.

25. MMLU. "Measuring Massive Multitask Language Understanding." UC Berkeley, 2021.

---

**AgentSDK版本**：v0.7.0
**白皮书版本**：1.0
**最后更新**：2025年11月
**许可证**：MIT License
**项目地址**：https://github.com/wordflowlab/agentsdk
**文档站点**：https://agentsdk.dev

---

## 参考快速链接

- [Memory Provenance](/memory/provenance)
- [Memory Consolidation](/memory/consolidation)
- [PII Redaction](/middleware/builtin/pii-redaction)
- [Context Window Management](/core-concepts/context-window)
- [Context Engineering Guide](/guides/advanced/context-engineering)
- [Session Sharing](/memory/session-sharing)
- [Session Compression](/memory/session-compression)
- [User Preferences](/memory/user-preferences)
