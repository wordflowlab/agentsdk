---
title: ç›‘æ§è¿ç»´
description: Agent åº”ç”¨çš„å¯è§‚æµ‹æ€§å’Œè¿ç»´å®è·µ
navigation:
  icon: i-lucide-bar-chart
---

# ç›‘æ§è¿ç»´æœ€ä½³å®è·µ

å®Œå–„çš„ç›‘æ§ç³»ç»Ÿæ˜¯ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œçš„åŸºç¡€ã€‚

## ğŸ¯ å¯è§‚æµ‹æ€§ä¸‰æ”¯æŸ±

```mermaid
graph LR
    A[å¯è§‚æµ‹æ€§] --> B[æ—¥å¿— Logging]
    A --> C[æŒ‡æ ‡ Metrics]
    A --> D[è¿½è¸ª Tracing]

    B --> B1[é”™è¯¯æ—¥å¿—]
    B --> B2[æ“ä½œæ—¥å¿—]
    B --> B3[å®¡è®¡æ—¥å¿—]

    C --> C1[ä¸šåŠ¡æŒ‡æ ‡]
    C --> C2[ç³»ç»ŸæŒ‡æ ‡]
    C --> C3[è‡ªå®šä¹‰æŒ‡æ ‡]

    D --> D1[è¯·æ±‚è¿½è¸ª]
    D --> D2[æ€§èƒ½åˆ†æ]
    D --> D3[ä¾èµ–å…³ç³»]
```

## ğŸ“ æ—¥å¿—è®°å½•

### ç»“æ„åŒ–æ—¥å¿—

```go
// âœ… ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—åº“ (å¦‚ zap, logrus)
import "go.uber.org/zap"

// åˆå§‹åŒ–æ—¥å¿—
func initLogger() *zap.Logger {
    config := zap.NewProductionConfig()
    config.OutputPaths = []string{"stdout", "/var/log/agent/app.log"}
    config.ErrorOutputPaths = []string{"stderr", "/var/log/agent/error.log"}

    logger, _ := config.Build()
    return logger
}

var logger = initLogger()

// ä½¿ç”¨ç»“æ„åŒ–å­—æ®µ
logger.Info("Agent created",
    zap.String("agent_id", agentID),
    zap.String("template_id", templateID),
    zap.String("user_id", userID),
    zap.Duration("init_time", initDuration),
)

logger.Error("Tool execution failed",
    zap.String("agent_id", agentID),
    zap.String("tool_name", toolName),
    zap.Error(err),
    zap.Any("input", toolInput),
)

// âŒ éç»“æ„åŒ–æ—¥å¿—
log.Printf("Agent %s created by user %s", agentID, userID)  // éš¾ä»¥è§£æ
```

### æ—¥å¿—çº§åˆ«

```go
// æ—¥å¿—çº§åˆ«ä½¿ç”¨è§„èŒƒ
type LogLevel int

const (
    DEBUG LogLevel = iota  // è°ƒè¯•ä¿¡æ¯ï¼Œä»…å¼€å‘ç¯å¢ƒ
    INFO                   // é‡è¦äº‹ä»¶ï¼Œæ­£å¸¸è¿è¡Œ
    WARN                   // è­¦å‘Šï¼Œæ½œåœ¨é—®é¢˜
    ERROR                  // é”™è¯¯ï¼Œéœ€è¦å…³æ³¨
    FATAL                  // è‡´å‘½é”™è¯¯ï¼Œç¨‹åºé€€å‡º
)

// ç¤ºä¾‹
func (ag *Agent) Chat(ctx context.Context, message string) (*types.ModelResponse, error) {
    // DEBUG - è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
    logger.Debug("Processing chat request",
        zap.String("agent_id", ag.ID()),
        zap.String("message", message[:min(50, len(message))]),
    )

    // INFO - é‡è¦çš„ä¸šåŠ¡äº‹ä»¶
    logger.Info("Chat started",
        zap.String("agent_id", ag.ID()),
        zap.Int("message_count", ag.MessageCount()),
    )

    // WARN - æ½œåœ¨é—®é¢˜
    if ag.TokenUsage() > 0.8*ag.TokenLimit() {
        logger.Warn("Token usage approaching limit",
            zap.String("agent_id", ag.ID()),
            zap.Int("used", ag.TokenUsage()),
            zap.Int("limit", ag.TokenLimit()),
        )
    }

    // ERROR - éœ€è¦å¤„ç†çš„é”™è¯¯
    result, err := ag.callModel(ctx, message)
    if err != nil {
        logger.Error("Model call failed",
            zap.String("agent_id", ag.ID()),
            zap.Error(err),
        )
        return nil, err
    }

    return result, nil
}
```

### æ—¥å¿—ä¸Šä¸‹æ–‡

```go
// âœ… æ·»åŠ è¯·æ±‚è¿½è¸ª ID
type contextKey string

const (
    requestIDKey contextKey = "request_id"
    userIDKey    contextKey = "user_id"
    tenantIDKey  contextKey = "tenant_id"
)

// ç”Ÿæˆè¯·æ±‚ ID
func withRequestID(ctx context.Context) context.Context {
    requestID := uuid.New().String()
    return context.WithValue(ctx, requestIDKey, requestID)
}

// ä»ä¸Šä¸‹æ–‡ä¸­æå–å­—æ®µ
func logWithContext(ctx context.Context, msg string, fields ...zap.Field) {
    contextFields := []zap.Field{
        zap.String("request_id", getRequestID(ctx)),
        zap.String("user_id", getUserID(ctx)),
        zap.String("tenant_id", getTenantID(ctx)),
    }
    allFields := append(contextFields, fields...)
    logger.Info(msg, allFields...)
}

// ä½¿ç”¨
func handleRequest(ctx context.Context) {
    ctx = withRequestID(ctx)

    logWithContext(ctx, "Request started")
    // æ‰€æœ‰æ—¥å¿—è‡ªåŠ¨åŒ…å« request_id, user_id, tenant_id
}
```

### æ—¥å¿—é‡‡æ ·

```go
// âœ… é«˜é¢‘æ—¥å¿—é‡‡æ ·ï¼Œé¿å…æ—¥å¿—æ´ªæ°´
import "go.uber.org/zap/zapcore"

// é…ç½®é‡‡æ ·æ—¥å¿—
func newSampledLogger() *zap.Logger {
    config := zap.NewProductionConfig()

    // æ¯ç§’è®°å½•å‰ 100 æ¡ï¼Œä¹‹åæ¯ 100 æ¡è®°å½• 1 æ¡
    config.Sampling = &zap.SamplingConfig{
        Initial:    100,
        Thereafter: 100,
    }

    logger, _ := config.Build()
    return logger
}

// é«˜é¢‘æ“ä½œä½¿ç”¨é‡‡æ ·æ—¥å¿—
sampledLogger := newSampledLogger()

for i := 0; i < 10000; i++ {
    sampledLogger.Debug("Tool call",  // ä¸ä¼šè®°å½•å…¨éƒ¨ 10000 æ¡
        zap.Int("index", i),
    )
}
```

## ğŸ“Š æŒ‡æ ‡æ”¶é›†

### æ ¸å¿ƒæŒ‡æ ‡

```go
// æŒ‡æ ‡ç±»å‹
type MetricsCollector struct {
    // Counter - è®¡æ•°å™¨ï¼Œåªå¢ä¸å‡
    agentCreated      prometheus.Counter
    toolCallsTotal    *prometheus.CounterVec
    errorsTotal       *prometheus.CounterVec

    // Gauge - ä»ªè¡¨ç›˜ï¼Œå¯å¢å¯å‡
    activeAgents      prometheus.Gauge
    poolSize          prometheus.Gauge
    memoryUsage       prometheus.Gauge

    // Histogram - ç›´æ–¹å›¾ï¼Œè®°å½•åˆ†å¸ƒ
    chatLatency       *prometheus.HistogramVec
    toolCallLatency   *prometheus.HistogramVec
    tokenUsage        *prometheus.HistogramVec

    // Summary - æ‘˜è¦ï¼Œè®¡ç®—ç™¾åˆ†ä½æ•°
    requestDuration   *prometheus.SummaryVec
}

// åˆå§‹åŒ–æŒ‡æ ‡
func NewMetricsCollector() *MetricsCollector {
    m := &MetricsCollector{
        // Counter
        agentCreated: prometheus.NewCounter(prometheus.CounterOpts{
            Name: "agent_created_total",
            Help: "Total number of agents created",
        }),

        toolCallsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "agent_tool_calls_total",
                Help: "Total number of tool calls",
            },
            []string{"tool_name", "status"},  // Labels
        ),

        errorsTotal: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "agent_errors_total",
                Help: "Total number of errors",
            },
            []string{"error_type", "agent_id"},
        ),

        // Gauge
        activeAgents: prometheus.NewGauge(prometheus.GaugeOpts{
            Name: "agent_active_count",
            Help: "Number of active agents",
        }),

        poolSize: prometheus.NewGauge(prometheus.GaugeOpts{
            Name: "agent_pool_size",
            Help: "Current size of agent pool",
        }),

        // Histogram
        chatLatency: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "agent_chat_latency_seconds",
                Help:    "Chat request latency",
                Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30},  // ç§’
            },
            []string{"agent_id", "template_id"},
        ),

        tokenUsage: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "agent_token_usage",
                Help:    "Token usage per request",
                Buckets: []float64{100, 500, 1000, 5000, 10000, 50000},
            },
            []string{"type"},  // input/output
        ),
    }

    // æ³¨å†Œæ‰€æœ‰æŒ‡æ ‡
    prometheus.MustRegister(
        m.agentCreated,
        m.toolCallsTotal,
        m.errorsTotal,
        m.activeAgents,
        m.poolSize,
        m.chatLatency,
        m.tokenUsage,
    )

    return m
}

// è®°å½•æŒ‡æ ‡
func (m *MetricsCollector) RecordAgentCreated() {
    m.agentCreated.Inc()
    m.activeAgents.Inc()
}

func (m *MetricsCollector) RecordAgentClosed() {
    m.activeAgents.Dec()
}

func (m *MetricsCollector) RecordToolCall(toolName string, success bool, duration time.Duration) {
    status := "success"
    if !success {
        status = "failed"
    }
    m.toolCallsTotal.WithLabelValues(toolName, status).Inc()
}

func (m *MetricsCollector) RecordChatLatency(agentID, templateID string, duration time.Duration) {
    m.chatLatency.WithLabelValues(agentID, templateID).Observe(duration.Seconds())
}

func (m *MetricsCollector) RecordTokenUsage(inputTokens, outputTokens int) {
    m.tokenUsage.WithLabelValues("input").Observe(float64(inputTokens))
    m.tokenUsage.WithLabelValues("output").Observe(float64(outputTokens))
}
```

### ä¸šåŠ¡æŒ‡æ ‡

```go
// ä¸šåŠ¡æŒ‡æ ‡ç¤ºä¾‹
type BusinessMetrics struct {
    // ç”¨æˆ·æ´»è·ƒåº¦
    dailyActiveUsers  prometheus.Gauge
    monthlyActiveUsers prometheus.Gauge

    // æˆæœ¬æŒ‡æ ‡
    dailyCost         prometheus.Counter
    costPerUser       *prometheus.GaugeVec

    // è´¨é‡æŒ‡æ ‡
    successRate       *prometheus.GaugeVec
    avgResponseTime   *prometheus.GaugeVec

    // å®¹é‡æŒ‡æ ‡
    concurrentUsers   prometheus.Gauge
    queueLength       prometheus.Gauge
}

// è®°å½•ä¸šåŠ¡æŒ‡æ ‡
func recordBusinessMetrics(ag *agent.Agent, result *types.ModelResponse) {
    // æˆæœ¬
    cost := calculateCost(result.InputTokens, result.OutputTokens)
    businessMetrics.dailyCost.Add(cost)

    userID := ag.UserID()
    businessMetrics.costPerUser.WithLabelValues(userID).Add(cost)

    // è´¨é‡
    if result.StopReason == "end_turn" {
        businessMetrics.successRate.WithLabelValues(ag.TemplateID()).Set(1.0)
    }

    // å“åº”æ—¶é—´
    businessMetrics.avgResponseTime.WithLabelValues(ag.TemplateID()).
        Set(result.Latency.Seconds())
}
```

### è‡ªå®šä¹‰æŒ‡æ ‡ä¸­é—´ä»¶

```go
// æŒ‡æ ‡æ”¶é›†ä¸­é—´ä»¶
type MetricsMiddleware struct {
    *middleware.BaseMiddleware
    collector *MetricsCollector
}

func NewMetricsMiddleware(collector *MetricsCollector) *MetricsMiddleware {
    return &MetricsMiddleware{
        BaseMiddleware: middleware.NewBaseMiddleware(
            "metrics",
            10,  // High priority
            []tools.Tool{},
        ),
        collector: collector,
    }
}

func (m *MetricsMiddleware) OnAgentStart(ctx context.Context, agentID string) error {
    m.collector.RecordAgentCreated()
    return nil
}

func (m *MetricsMiddleware) OnAgentStop(ctx context.Context, agentID string) error {
    m.collector.RecordAgentClosed()
    return nil
}

func (m *MetricsMiddleware) WrapModelCall(
    ctx context.Context,
    req *types.ModelRequest,
    handler middleware.ModelCallHandler,
) (*types.ModelResponse, error) {
    start := time.Now()

    resp, err := handler(ctx, req)

    duration := time.Since(start)
    m.collector.RecordChatLatency(req.AgentID, req.TemplateID, duration)

    if err != nil {
        m.collector.errorsTotal.WithLabelValues("model_call", req.AgentID).Inc()
    } else {
        m.collector.RecordTokenUsage(resp.InputTokens, resp.OutputTokens)
    }

    return resp, err
}

func (m *MetricsMiddleware) WrapToolCall(
    ctx context.Context,
    req *types.ToolCallRequest,
    handler middleware.ToolCallHandler,
) (*types.ToolCallResponse, error) {
    start := time.Now()

    resp, err := handler(ctx, req)

    duration := time.Since(start)
    success := err == nil
    m.collector.RecordToolCall(req.ToolName, success, duration)

    return resp, err
}
```

### æš´éœ²æŒ‡æ ‡ç«¯ç‚¹

```go
import (
    "net/http"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

// å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
func startMetricsServer(port int) {
    http.Handle("/metrics", promhttp.Handler())

    // å¥åº·æ£€æŸ¥ç«¯ç‚¹
    http.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("OK"))
    })

    // å°±ç»ªæ£€æŸ¥ç«¯ç‚¹
    http.HandleFunc("/ready", func(w http.ResponseWriter, r *http.Request) {
        if isSystemReady() {
            w.WriteHeader(http.StatusOK)
            w.Write([]byte("Ready"))
        } else {
            w.WriteHeader(http.StatusServiceUnavailable)
            w.Write([]byte("Not Ready"))
        }
    })

    addr := fmt.Sprintf(":%d", port)
    log.Printf("Metrics server listening on %s", addr)
    http.ListenAndServe(addr, nil)
}

// åœ¨ main å‡½æ•°ä¸­å¯åŠ¨
func main() {
    // å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
    go startMetricsServer(9090)

    // å¯åŠ¨åº”ç”¨...
}
```

## ğŸ” åˆ†å¸ƒå¼è¿½è¸ª

### OpenTelemetry é›†æˆ

```go
import (
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/trace"
    "go.opentelemetry.io/otel/attribute"
)

// åˆå§‹åŒ–è¿½è¸ª
func initTracer() trace.Tracer {
    return otel.Tracer("agentsdk")
}

var tracer = initTracer()

// åœ¨å…³é”®è·¯å¾„æ·»åŠ è¿½è¸ª
func (ag *Agent) Chat(ctx context.Context, message string) (*types.ModelResponse, error) {
    // åˆ›å»º Span
    ctx, span := tracer.Start(ctx, "Agent.Chat",
        trace.WithAttributes(
            attribute.String("agent.id", ag.ID()),
            attribute.String("agent.template", ag.TemplateID()),
            attribute.Int("message.length", len(message)),
        ),
    )
    defer span.End()

    // ä¸šåŠ¡é€»è¾‘
    result, err := ag.processChat(ctx, message)

    // è®°å½•ç»“æœ
    if err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
    } else {
        span.SetAttributes(
            attribute.Int("tokens.input", result.InputTokens),
            attribute.Int("tokens.output", result.OutputTokens),
        )
    }

    return result, err
}

// å·¥å…·è°ƒç”¨è¿½è¸ª
func (t *Tool) Execute(ctx context.Context, input map[string]interface{}, tc *tools.ToolContext) (interface{}, error) {
    ctx, span := tracer.Start(ctx, fmt.Sprintf("Tool.%s", t.Name()),
        trace.WithAttributes(
            attribute.String("tool.name", t.Name()),
            attribute.String("tool.input", fmt.Sprintf("%v", input)),
        ),
    )
    defer span.End()

    result, err := t.execute(ctx, input, tc)

    if err != nil {
        span.RecordError(err)
    }

    return result, err
}
```

### è¿½è¸ªå¯è§†åŒ–

```mermaid
gantt
    title Agent Chat è¯·æ±‚è¿½è¸ª
    dateFormat  HH:mm:ss.SSS
    axisFormat  %M:%S

    section Agent
    Chat Request     :active, 00:00:00.000, 5s

    section Middleware
    Auth Check       :00:00:00.010, 0.1s
    Rate Limit       :00:00:00.110, 0.05s

    section Model Call
    Prepare Request  :00:00:00.160, 0.2s
    API Call         :00:00:00.360, 3s
    Parse Response   :00:00:03.360, 0.1s

    section Tool Calls
    Read          :00:00:03.460, 0.3s
    Bash         :00:00:03.760, 1s

    section Response
    Format Response  :00:00:04.760, 0.1s
```

## ğŸ”” å‘Šè­¦é…ç½®

### Prometheus å‘Šè­¦è§„åˆ™

```yaml
# prometheus-rules.yml
groups:
  - name: agent_alerts
    interval: 30s
    rules:
      # é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: |
          rate(agent_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Agent error rate is high"
          description: "Error rate is {{ $value }} errors/sec for agent {{ $labels.agent_id }}"

      # å“åº”æ—¶é—´å‘Šè­¦
      - alert: SlowResponse
        expr: |
          histogram_quantile(0.95, rate(agent_chat_latency_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Agent response time is slow"
          description: "P95 latency is {{ $value }}s for template {{ $labels.template_id }}"

      # Token ä½¿ç”¨å‘Šè­¦
      - alert: HighTokenUsage
        expr: |
          rate(agent_token_usage_sum[1h]) > 1000000
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "High token usage detected"
          description: "Token usage rate is {{ $value }} tokens/sec"

      # Agent Pool å®¹é‡å‘Šè­¦
      - alert: PoolNearCapacity
        expr: |
          agent_pool_size / agent_pool_max_size > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Agent pool near capacity"
          description: "Pool is {{ $value | humanizePercentage }} full"

      # å†…å­˜ä½¿ç”¨å‘Šè­¦
      - alert: HighMemoryUsage
        expr: |
          agent_memory_usage_bytes > 1e9
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize }}B"

      # API è°ƒç”¨å¤±è´¥
      - alert: APICallFailures
        expr: |
          increase(agent_tool_calls_total{status="failed"}[5m]) > 10
        labels:
          severity: warning
        annotations:
          summary: "Multiple API call failures"
          description: "{{ $value }} failed calls for tool {{ $labels.tool_name }}"
```

### AlertManager é…ç½®

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # ä¸¥é‡å‘Šè­¦ç«‹å³å‘é€
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true

    # è­¦å‘Šå‘é€åˆ° Slack
    - match:
        severity: warning
      receiver: 'slack'

    # ä¿¡æ¯åªè®°å½•
    - match:
        severity: info
      receiver: 'log'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'Agent Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'

  - name: 'slack'
    slack_configs:
      - channel: '#agent-warnings'
        title: 'Agent Warning'

  - name: 'log'
    webhook_configs:
      - url: 'http://logger:8080/alerts'
```

### è‡ªå®šä¹‰å‘Šè­¦å¤„ç†

```go
// å‘Šè­¦å¤„ç†å™¨
type AlertHandler struct {
    notifiers []Notifier
}

type Notifier interface {
    Notify(alert *Alert) error
}

type Alert struct {
    Name        string
    Severity    string
    Description string
    Labels      map[string]string
    Value       float64
    Timestamp   time.Time
}

// Slack é€šçŸ¥å™¨
type SlackNotifier struct {
    webhookURL string
}

func (n *SlackNotifier) Notify(alert *Alert) error {
    message := map[string]interface{}{
        "text": fmt.Sprintf("ğŸš¨ *%s*\n%s\nValue: %.2f",
            alert.Name, alert.Description, alert.Value),
        "attachments": []map[string]interface{}{
            {
                "color": getSeverityColor(alert.Severity),
                "fields": []map[string]interface{}{
                    {"title": "Severity", "value": alert.Severity, "short": true},
                    {"title": "Time", "value": alert.Timestamp.Format(time.RFC3339), "short": true},
                },
            },
        },
    }

    // å‘é€åˆ° Slack
    return sendSlackMessage(n.webhookURL, message)
}

// å‘Šè­¦è§„åˆ™å¼•æ“
type AlertRuleEngine struct {
    rules   []*AlertRule
    handler *AlertHandler
}

type AlertRule struct {
    Name      string
    Condition func(*MetricsSnapshot) bool
    Severity  string
    Message   func(*MetricsSnapshot) string
}

func (e *AlertRuleEngine) Evaluate(snapshot *MetricsSnapshot) {
    for _, rule := range e.rules {
        if rule.Condition(snapshot) {
            alert := &Alert{
                Name:        rule.Name,
                Severity:    rule.Severity,
                Description: rule.Message(snapshot),
                Timestamp:   time.Now(),
            }
            e.handler.NotifyAll(alert)
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
func setupAlertRules() *AlertRuleEngine {
    engine := &AlertRuleEngine{
        handler: &AlertHandler{
            notifiers: []Notifier{
                &SlackNotifier{webhookURL: os.Getenv("SLACK_WEBHOOK")},
                &EmailNotifier{smtpConfig: emailConfig},
            },
        },
    }

    // æ·»åŠ å‘Šè­¦è§„åˆ™
    engine.AddRule(&AlertRule{
        Name:     "HighErrorRate",
        Severity: "warning",
        Condition: func(m *MetricsSnapshot) bool {
            return m.ErrorRate > 0.1
        },
        Message: func(m *MetricsSnapshot) string {
            return fmt.Sprintf("Error rate is %.2f%%", m.ErrorRate*100)
        },
    })

    return engine
}
```

## ğŸ“ˆ ç›‘æ§å¤§ç›˜

### Grafana Dashboard é…ç½®

```json
{
  "dashboard": {
    "title": "Agent SDK Monitoring",
    "panels": [
      {
        "title": "Agent åˆ›å»ºé€Ÿç‡",
        "targets": [
          {
            "expr": "rate(agent_created_total[5m])"
          }
        ],
        "type": "graph"
      },
      {
        "title": "æ´»è·ƒ Agent æ•°é‡",
        "targets": [
          {
            "expr": "agent_active_count"
          }
        ],
        "type": "stat"
      },
      {
        "title": "P95 å“åº”æ—¶é—´",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(agent_chat_latency_seconds_bucket[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Token ä½¿ç”¨é‡",
        "targets": [
          {
            "expr": "rate(agent_token_usage_sum[1h])",
            "legendFormat": "{{ type }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "å·¥å…·è°ƒç”¨åˆ†å¸ƒ",
        "targets": [
          {
            "expr": "sum(rate(agent_tool_calls_total[5m])) by (tool_name)"
          }
        ],
        "type": "piechart"
      },
      {
        "title": "é”™è¯¯ç‡",
        "targets": [
          {
            "expr": "rate(agent_errors_total[5m])",
            "legendFormat": "{{ error_type }}"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

## ğŸ”§ é—®é¢˜æ’æŸ¥

### æ—¥å¿—æŸ¥è¯¢ç¤ºä¾‹

```bash
# æŸ¥è¯¢ç‰¹å®š Agent çš„æ—¥å¿—
grep "agent_id=abc123" /var/log/agent/app.log

# æŸ¥è¯¢é”™è¯¯æ—¥å¿—
jq 'select(.level == "error")' /var/log/agent/app.log

# æŸ¥è¯¢ç‰¹å®šæ—¶é—´èŒƒå›´
jq 'select(.timestamp >= "2024-01-01T00:00:00Z" and .timestamp <= "2024-01-02T00:00:00Z")' \
   /var/log/agent/app.log

# æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„ç»Ÿè®¡
jq -r 'select(.level == "error") | .error_type' /var/log/agent/app.log | \
   sort | uniq -c | sort -nr
```

### å¸¸è§é—®é¢˜è¯Šæ–­

```go
// è¯Šæ–­å·¥å…·
type DiagnosticTool struct {
    pool      *core.Pool
    metrics   *MetricsCollector
    logger    *zap.Logger
}

// ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
func (d *DiagnosticTool) GenerateReport() *DiagnosticReport {
    report := &DiagnosticReport{
        Timestamp: time.Now(),
    }

    // 1. Agent çŠ¶æ€
    report.PoolSize = d.pool.Size()
    report.ActiveAgents = d.getActiveAgents()

    // 2. èµ„æºä½¿ç”¨
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    report.MemoryUsage = m.Alloc
    report.Goroutines = runtime.NumGoroutine()

    // 3. é”™è¯¯ç»Ÿè®¡
    report.RecentErrors = d.getRecentErrors(1 * time.Hour)

    // 4. æ€§èƒ½æŒ‡æ ‡
    report.AvgLatency = d.getAverageLatency()
    report.P95Latency = d.getP95Latency()

    return report
}

// å¥åº·æ£€æŸ¥
func (d *DiagnosticTool) HealthCheck() *HealthStatus {
    status := &HealthStatus{
        Healthy: true,
        Checks:  make(map[string]bool),
    }

    // æ£€æŸ¥ Pool
    status.Checks["pool"] = d.pool.Size() < d.pool.MaxSize()

    // æ£€æŸ¥å†…å­˜
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    status.Checks["memory"] = m.Alloc < 2*1024*1024*1024  // < 2GB

    // æ£€æŸ¥é”™è¯¯ç‡
    errorRate := d.metrics.GetErrorRate()
    status.Checks["error_rate"] = errorRate < 0.1

    // æ•´ä½“å¥åº·
    for _, check := range status.Checks {
        if !check {
            status.Healthy = false
            break
        }
    }

    return status
}
```

## âœ… ç›‘æ§æ£€æŸ¥æ¸…å•

### ä¸Šçº¿å‰

- [ ] é…ç½®ç»“æ„åŒ–æ—¥å¿—
- [ ] è®¾ç½®æ—¥å¿—è½®è½¬å’Œå½’æ¡£
- [ ] éƒ¨ç½² Prometheus å’Œ Grafana
- [ ] é…ç½®æ ¸å¿ƒæŒ‡æ ‡æ”¶é›†
- [ ] è®¾ç½®å‘Šè­¦è§„åˆ™
- [ ] é…ç½®å‘Šè­¦é€šçŸ¥æ¸ é“
- [ ] åˆ›å»ºç›‘æ§å¤§ç›˜
- [ ] è®¾ç½®æ—¥å¿—èšåˆï¼ˆå¦‚ ELKï¼‰
- [ ] é…ç½®åˆ†å¸ƒå¼è¿½è¸ªï¼ˆå¯é€‰ï¼‰
- [ ] ç¼–å†™è¿ç»´æ–‡æ¡£

### æ—¥å¸¸è¿ç»´

- [ ] æ¯æ—¥æ£€æŸ¥ç›‘æ§å¤§ç›˜
- [ ] å®¡æŸ¥å‘Šè­¦å’Œé”™è¯¯æ—¥å¿—
- [ ] ç›‘æ§èµ„æºä½¿ç”¨è¶‹åŠ¿
- [ ] è·Ÿè¸ªæˆæœ¬æŒ‡æ ‡
- [ ] å®šæœŸç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
- [ ] ä¼˜åŒ–æ…¢æŸ¥è¯¢å’Œæ€§èƒ½ç“¶é¢ˆ
- [ ] æ›´æ–°å‘Šè­¦é˜ˆå€¼

## ğŸ”— ç›¸å…³èµ„æº

- [é”™è¯¯å¤„ç†](/best-practices/error-handling)
- [æ€§èƒ½ä¼˜åŒ–](/best-practices/performance)
- [éƒ¨ç½²å®è·µ](/best-practices/deployment)
- [Prometheus æ–‡æ¡£](https://prometheus.io/docs/)
- [Grafana æ–‡æ¡£](https://grafana.com/docs/)
- [OpenTelemetry æ–‡æ¡£](https://opentelemetry.io/docs/)
